{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Main Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import json # Export dictionary of tuned parameters.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV # For hyperparameter tuning.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier, StackingClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation metrics import\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    balanced_accuracy_score, \n",
    "    f1_score, \n",
    "    roc_auc_score, \n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Models import\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Visualisations import\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for data, which is most important split.\n",
    "RANDOM_STATE = 42 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1 Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2 Workflow related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2 Hyperparameter tuning workflow related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow related functions for each model hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization related function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion, labels, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plots a heatmap for the given confusion matrix with annotations.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load a dataset from a specified file path. Supports gzip-compressed files.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split(data, label_col, random_state=42, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and testing sets without preprocessing.\n",
    "    \"\"\"\n",
    "    X = data.drop(columns=[label_col]).values\n",
    "    y = LabelEncoder().fit_transform(data[label_col].values)\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(model, model_name, standardize=False, with_pca=False, n_pca_components=10):\n",
    "    \"\"\"\n",
    "    Creates a pipeline for a given model with optional standardization and PCA.\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    if standardize:\n",
    "        steps.append(('scaler', StandardScaler()))\n",
    "    if with_pca:\n",
    "        steps.append(('pca', PCA(n_components=n_pca_components)))\n",
    "\n",
    "    if \"random_state\" in model.get_params():\n",
    "        model.set_params(random_state=42)\n",
    "\n",
    "    steps.append((model_name, model))\n",
    "    pipeline = Pipeline(steps)\n",
    "    print(f\"Pipeline Steps ({model_name}):\", pipeline.steps)  # Debug pipeline steps\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning for a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_tuning(model_name, config, X_train, y_train, scoring=\"f1\", cv_folds=5, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for a model pipeline using GridSearchCV.\n",
    "    \"\"\"\n",
    "    pipeline = create_pipeline(\n",
    "        model=config[\"model\"],\n",
    "        model_name=model_name,\n",
    "        standardize=config[\"preprocess\"].get(\"standardize\", False),\n",
    "        with_pca=config[\"preprocess\"].get(\"pca\", False),\n",
    "        n_pca_components=config[\"preprocess\"].get(\"pca_components\", 10)\n",
    "    )\n",
    "    pipeline.set_params(**config[\"fixed_params\"])\n",
    "    \n",
    "    # Grid Search Configuration\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid=config[\"grid_search_params\"],\n",
    "        scoring=scoring,\n",
    "        # Cross-validation with fixed random state\n",
    "        cv=StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42),\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=0\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    elapsed_time = (time.time() - start_time) / 60\n",
    "    \n",
    "    # Save Best Parameters and Results\n",
    "    best_params = grid_search.best_params_\n",
    "    best_params[\"MODEL_NAME\"] = model_name\n",
    "    best_params[\"TIME_ELAPSED_MIN\"] = elapsed_time\n",
    "\n",
    "    # Convert non-serializable objects in best_params to strings\n",
    "    for key, value in best_params.items():\n",
    "        if not isinstance(value, (str, int, float, list, dict, bool, type(None))):\n",
    "            best_params[key] = str(value)\n",
    "\n",
    "    full_results = pd.DataFrame(grid_search.cv_results_)\n",
    "    full_results.to_csv(f\"Tuning_params/{model_name}_all_results.csv\", index=False)\n",
    "\n",
    "    with open(f\"Tuning_params/{model_name}_best_params.json\", \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "\n",
    "    print(f\"Model {model_name} tuned in {elapsed_time:.2f} minutes\")\n",
    "    return grid_search, best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate optimised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_optimized_model(model_name, model_config, results_dir, X_train, X_test, y_train, y_test, source_model_name=None):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a single optimized model and saves results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use source model name for loading best parameters if provided\n",
    "        param_source_name = source_model_name or model_name\n",
    "\n",
    "        # Load Best Parameters\n",
    "        with open(f\"{results_dir}/{param_source_name}_best_params.json\", \"r\") as f:\n",
    "            best_params = json.load(f)\n",
    "        \n",
    "        # Ensure random_state is consistent across all steps\n",
    "        if \"random_state\" in model_config[\"model\"].get_params():\n",
    "            model_config[\"model\"].set_params(random_state=42)\n",
    "        print(\"Loaded Best Parameters:\", best_params)\n",
    "        \n",
    "        pipeline_params = {k: v for k, v in best_params.items() if \"__\" in k}\n",
    "        print(\"Pipeline Parameters to Set:\", pipeline_params)\n",
    "\n",
    "        # Create Pipeline\n",
    "        pipeline = create_pipeline(\n",
    "            model=model_config[\"model\"],\n",
    "            model_name=model_name,\n",
    "            standardize=model_config[\"preprocess\"].get(\"standardize\", False),\n",
    "            with_pca=model_config[\"preprocess\"].get(\"pca\", False),\n",
    "            n_pca_components=model_config[\"preprocess\"].get(\"pca_components\", 10)\n",
    "        )\n",
    "        print(\"Pipeline Before Setting Params:\", pipeline.steps)\n",
    "        pipeline.set_params(**pipeline_params)\n",
    "        print(\"Pipeline After Setting Params:\", pipeline.steps)\n",
    "\n",
    "        # Fit Pipeline to Training Data\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate on Test Data\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc_score = roc_auc_score(y_test, pipeline.predict_proba(X_test)[:, 1]) if hasattr(pipeline, \"predict_proba\") else None\n",
    "        confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        results = {\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Balanced Accuracy\": balanced_accuracy,\n",
    "            \"AUC\": auc_score,\n",
    "            \"F1 Score\": f1,\n",
    "            \"Confusion Matrix\": confusion.tolist()\n",
    "        }\n",
    "\n",
    "        # Save Results\n",
    "        pd.DataFrame([results]).to_csv(f\"{results_dir}/{model_name}_evaluation_summary.csv\", index=False)\n",
    "        print(f\"Evaluation results for {model_name} saved in file {results_dir}/{model_name}_evaluation_summary.csv\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model {model_name}: {e}\")\n",
    "        return {\"Model\": model_name, \"Error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning and Evaluation stage (All models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning LDA no PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Steps (lda): [('scaler', StandardScaler()), ('lda', LinearDiscriminantAnalysis())]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ST443/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lda tuned in 5.62 minutes\n"
     ]
    }
   ],
   "source": [
    "# LDA Configuration\n",
    "lda_config = {\n",
    "    \"model\": LinearDiscriminantAnalysis(),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": False},\n",
    "    \"fixed_params\": {\"lda__tol\": 0.0001},\n",
    "    \"grid_search_params\": [\n",
    "        {\"lda__solver\": [\"svd\"]},\n",
    "        {\"lda__solver\": [\"lsqr\"], \"lda__shrinkage\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \"auto\"]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Tune LDA and get the best params\n",
    "lda_results, lda_best_params = grid_search_tuning(\"lda\", lda_config, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for LDA without PCA:\n",
      "{'lda__shrinkage': 0.8, 'lda__solver': 'lsqr', 'MODEL_NAME': 'lda', 'TIME_ELAPSED_MIN': 5.621273163954417}\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters used after tuning\n",
    "print(\"Best Parameters for LDA without PCA:\")\n",
    "print(lda_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results for LDA without PCA:\n",
      "Loaded Best Parameters: {'lda__shrinkage': 0.8, 'lda__solver': 'lsqr', 'MODEL_NAME': 'lda', 'TIME_ELAPSED_MIN': 5.621273163954417}\n",
      "Pipeline Parameters to Set: {'lda__shrinkage': 0.8, 'lda__solver': 'lsqr'}\n",
      "Pipeline Steps (lda): [('scaler', StandardScaler()), ('lda', LinearDiscriminantAnalysis())]\n",
      "Pipeline Before Setting Params: [('scaler', StandardScaler()), ('lda', LinearDiscriminantAnalysis())]\n",
      "Pipeline After Setting Params: [('scaler', StandardScaler()), ('lda', LinearDiscriminantAnalysis(shrinkage=0.8, solver='lsqr'))]\n",
      "Evaluation results for lda saved in file Tuning_params/lda_evaluation_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': 'lda',\n",
       " 'Accuracy': 0.965296803652968,\n",
       " 'Balanced Accuracy': 0.95587000064612,\n",
       " 'AUC': 0.9932336872635381,\n",
       " 'F1 Score': 0.9511568123393316,\n",
       " 'Confusion Matrix': [[687, 6], [32, 370]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate LDA with best params\n",
    "print(\"Evaluation Results for LDA without PCA:\")\n",
    "lda_eval = evaluate_optimized_model(\"lda\", lda_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "lda_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print confusion matrix\n",
    "if \"Confusion Matrix\" in lda_eval:\n",
    "    labels = sorted(np.unique(y_test))  # Ensure proper labels are used\n",
    "    plot_confusion_matrix(lda_eval[\"Confusion Matrix\"], labels, title=\"LDA Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the best tuning params for LDA with PCA 10 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda_best_params: {'lda__shrinkage': 0.8, 'lda__solver': 'lsqr', 'MODEL_NAME': 'lda', 'TIME_ELAPSED_MIN': 5.621273163954417}\n",
      "Transformed Params for lda_pca: {'lda_pca__shrinkage': 0.8, 'lda_pca__solver': 'lsqr', 'lda_pca__tol': 0.0001}\n",
      "LDA PCA Config: {'model': LinearDiscriminantAnalysis(), 'preprocess': {'standardize': True, 'pca': True}, 'fixed_params': {'lda_pca__shrinkage': 0.8, 'lda_pca__solver': 'lsqr', 'lda_pca__tol': 0.0001}}\n",
      "Loaded Best Parameters: {'lda__shrinkage': 0.8, 'lda__solver': 'lsqr', 'MODEL_NAME': 'lda', 'TIME_ELAPSED_MIN': 5.621273163954417}\n",
      "Pipeline Parameters to Set: {'lda__shrinkage': 0.8, 'lda__solver': 'lsqr'}\n",
      "Pipeline Steps (lda_pca): [('scaler', StandardScaler()), ('pca', PCA(n_components=10)), ('lda_pca', LinearDiscriminantAnalysis())]\n",
      "Pipeline Before Setting Params: [('scaler', StandardScaler()), ('pca', PCA(n_components=10)), ('lda_pca', LinearDiscriminantAnalysis())]\n",
      "Error evaluating model lda_pca: Invalid parameter 'lda' for estimator Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=10)),\n",
      "                ('lda_pca', LinearDiscriminantAnalysis())]). Valid parameters are: ['memory', 'steps', 'verbose'].\n",
      "Evaluation Results for LDA with PCA:\n",
      "{'Model': 'lda_pca', 'Error': \"Invalid parameter 'lda' for estimator Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=10)),\\n                ('lda_pca', LinearDiscriminantAnalysis())]). Valid parameters are: ['memory', 'steps', 'verbose'].\"}\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters\n",
    "print(\"lda_best_params:\", lda_best_params)\n",
    "\n",
    "# Step 1: Transform parameters for the lda_pca step\n",
    "lda_pca_fixed_params = {\n",
    "    f\"lda_pca__{key.split('__')[1]}\": value for key, value in lda_best_params.items() if \"__\" in key\n",
    "}\n",
    "lda_pca_fixed_params[\"lda_pca__tol\"] = 0.0001  # Add additional fixed parameters manually\n",
    "print(\"Transformed Params for lda_pca:\", lda_pca_fixed_params)\n",
    "\n",
    "# Step 2: LDA Configuration for PCA\n",
    "lda_pca_config = {\n",
    "    \"model\": LinearDiscriminantAnalysis(),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": True},  # Enable PCA\n",
    "    \"fixed_params\": lda_pca_fixed_params\n",
    "}\n",
    "print(\"LDA PCA Config:\", lda_pca_config)\n",
    "\n",
    "# Step 3: Evaluate LDA With PCA\n",
    "lda_pca_eval = evaluate_optimized_model(\n",
    "    model_name=\"lda_pca\",\n",
    "    model_config=lda_pca_config,\n",
    "    results_dir=\"Tuning_params\",\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    source_model_name=\"lda\"  # Reuse parameters from LDA without PCA\n",
    ")\n",
    "\n",
    "# Step 4: Print Evaluation Results for LDA With PCA\n",
    "print(\"Evaluation Results for LDA with PCA:\")\n",
    "print(lda_pca_eval)\n",
    "\n",
    "# Step 5: Plot Confusion Matrix for LDA With PCA\n",
    "if \"Confusion Matrix\" in lda_pca_eval:\n",
    "    labels = sorted(np.unique(y_test))\n",
    "    plot_confusion_matrix(lda_pca_eval[\"Confusion Matrix\"], labels, title=\"LDA Confusion Matrix (With PCA, 10 Components)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Confusion Matrix\" in lda_pca_eval:\n",
    "    labels = sorted(np.unique(y_test))\n",
    "    plot_confusion_matrix(lda_pca_eval[\"Confusion Matrix\"], labels, title=\"LDA (PCA 10 Components) Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning logistic No PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# Logistic Regression Configuration\n",
    "logit_config = {\n",
    "    \"model\": LogisticRegression(random_state=42),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": False},\n",
    "    \"fixed_params\": {\n",
    "        \"logit__max_iter\": 100,\n",
    "        \"logit__solver\": \"saga\",\n",
    "        \"logit__n_jobs\": -1,\n",
    "        \"logit__penalty\": \"elasticnet\"\n",
    "    },\n",
    "    \"grid_search_params\": {\n",
    "        \"logit__C\": [0.001, 0.101, 0.201, 0.301, 0.401, 0.501, 0.601, 0.701, 0.801, 0.901, 1.001, 5, 10],\n",
    "        \"logit__l1_ratio\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        \"logit__class_weight\": [None, \"balanced\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune Logistic Regression\n",
    "logit_results, logit_best_params = grid_search_tuning(\"logit\", logit_config, X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Logistic Regression\n",
    "logit_eval = evaluate_optimized_model(\"logit\", logit_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "logit_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model hyperparameter tuning and evaluation workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion, labels, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plots a heatmap for the given confusion matrix with annotations.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load a dataset from a specified file path. Supports gzip-compressed files.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path, compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split(data, label_col, random_state=42, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and testing sets without preprocessing.\n",
    "    \"\"\"\n",
    "    X = data.drop(columns=[label_col]).values\n",
    "    y = LabelEncoder().fit_transform(data[label_col].values)\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPipeline:\n",
    "    def __init__(self, model, name, standardize=False, with_pca=False, n_pca_components=10, random_state=42):\n",
    "        \"\"\"\n",
    "        Constructor to initialize the pipeline for a given model.\n",
    "        - model: The machine learning model (e.g., LDA, RandomForest).\n",
    "        - name: A unique name for the model.\n",
    "        - standardize: Whether to apply StandardScaler.\n",
    "        - with_pca: Whether to include PCA.\n",
    "        - n_pca_components: Number of PCA components if PCA is used.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.standardize = standardize\n",
    "        self.with_pca = with_pca\n",
    "        self.n_pca_components = n_pca_components\n",
    "        self.pipeline = None\n",
    "        self.best_params = {}\n",
    "        self.results = {}  # Store evaluation results for each model\n",
    "\n",
    "        # Ensure model uses random_state if supported (fixing randomness)\n",
    "        if \"random_state\" in self.model.get_params():\n",
    "            self.model.set_params(random_state=self.random_state)\n",
    "\n",
    "    def create_pipeline(self):\n",
    "        \"\"\"\n",
    "        Assembles the pipeline steps dynamically based on the configuration:\n",
    "        - Adds StandardScaler if standardize=True.\n",
    "        - Adds PCA if with_pca=True.\n",
    "        - Adds the given model at the end.\n",
    "        \"\"\"\n",
    "        steps = []\n",
    "        if self.standardize:\n",
    "            steps.append(('scaler', StandardScaler()))\n",
    "        if self.with_pca:\n",
    "            steps.append(('pca', PCA(n_components=self.n_pca_components)))\n",
    "        steps.append((self.name, self.model))\n",
    "        self.pipeline = Pipeline(steps)\n",
    "        # print(f\"Pipeline Created: {self.pipeline}\")\n",
    "        return self.pipeline\n",
    "\n",
    "    def set_params(self, params):\n",
    "        \"\"\"\n",
    "        Updates the parameters of the pipeline.\n",
    "        - params: Dictionary of parameters to set.\n",
    "        \"\"\"\n",
    "        if not self.pipeline:\n",
    "            raise ValueError(\"Pipeline not created. Call create_pipeline() first.\")\n",
    "        self.pipeline.set_params(**params)\n",
    "\n",
    "    def tune(self, X_train, y_train, param_grid, scoring=\"f1\", cv_folds=5, n_jobs=-1):\n",
    "        \"\"\"\n",
    "        Performs hyperparameter tuning using GridSearchCV.\n",
    "        - X_train, y_train: Training data and labels.\n",
    "        - param_grid: Grid of hyperparameters to tune.\n",
    "        - scoring: base the scoring on the f1 metric\n",
    "        - cv_folds: Number of cross-validation folds.\n",
    "        - n_jobs: Number of parallel jobs for GridSearchCV (-1 means maximum possible)\n",
    "        \"\"\"\n",
    "        # Esnsure pipeline is created\n",
    "        if not self.pipeline:\n",
    "            self.create_pipeline() \n",
    "\n",
    "        # Setup GridSearchCV with the pipeline\n",
    "        grid_search = GridSearchCV(\n",
    "            self.pipeline,\n",
    "            param_grid=param_grid,\n",
    "            scoring=scoring,\n",
    "            cv=StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state),\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "\n",
    "        start_time = time.time() # Track tuning time\n",
    "        grid_search.fit(X_train, y_train) # Fit the grid search\n",
    "        elapsed_time = (time.time() - start_time) / 60 # Time in minutes\n",
    "\n",
    "        # Store the best parameters and save results\n",
    "        self.best_params = grid_search.best_params_\n",
    "        self.best_params[\"MODEL_NAME\"] = self.name\n",
    "        self.best_params[\"TIME_ELAPSED_MIN\"] = elapsed_time\n",
    "\n",
    "        # Save all results and best parameters\n",
    "        full_results = pd.DataFrame(grid_search.cv_results_)\n",
    "        full_results.to_csv(f\"Tuning_params/{self.name}_all_results.csv\", index=False)\n",
    "        with open(f\"Tuning_params/{self.name}_best_params.json\", \"w\") as f:\n",
    "            json.dump(self.best_params, f, indent=4)\n",
    "\n",
    "        print(f\"Model {self.name} tuned in {elapsed_time:.2f} minutes\") #log running time\n",
    "\n",
    "    def evaluate(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "        Evaluates the performance of the pipeline on test data.\n",
    "        - X_train, X_test: Training and testing feature matrices.\n",
    "        - y_train, y_test: Training and testing labels.\n",
    "        \"\"\"\n",
    "        if not self.pipeline:\n",
    "            self.create_pipeline()\n",
    "\n",
    "        if self.best_params:\n",
    "            # Filter only valid pipeline parameters\n",
    "            valid_params = {\n",
    "                key: value for key, value in self.best_params.items()\n",
    "                if key in self.pipeline.get_params()\n",
    "            }\n",
    "            print(f\"Valid Parameters for Pipeline: {valid_params}\")\n",
    "            self.pipeline.set_params(**valid_params) # Set valid parameters\n",
    "\n",
    "        # Train the pipeline on training data\n",
    "        self.pipeline.fit(X_train, y_train)\n",
    "        # Predict on test data\n",
    "        y_pred = self.pipeline.predict(X_test)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        metrics = {\n",
    "            \"Model\": self.name,\n",
    "            \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"Balanced Accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
    "            \"F1 Score\": f1_score(y_test, y_pred),\n",
    "        }\n",
    "\n",
    "        # Calculate AUC if available\n",
    "        if hasattr(self.pipeline, \"predict_proba\"):\n",
    "            metrics[\"AUC\"] = roc_auc_score(y_test, self.pipeline.predict_proba(X_test)[:, 1])\n",
    "        else:\n",
    "            metrics[\"AUC\"] = None\n",
    "\n",
    "        # Add confusion matrix (fir plot)\n",
    "        metrics[\"Confusion Matrix\"] = confusion_matrix(y_test, y_pred).tolist()\n",
    "        print(f\"Evaluation Results: {metrics}\")\n",
    "\n",
    "        # # Save evaluation metrics\n",
    "        self.results = metrics\n",
    "        pd.DataFrame([metrics]).to_csv(f\"Tuning_params/{self.name}_evaluation_summary.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_results(models):\n",
    "    \"\"\"\n",
    "    Aggregates results from multiple ModelPipeline instances into a single DataFrame (To merge all models' results)\n",
    "    \"\"\"\n",
    "    all_results = [model.results for model in models]\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_csv(\"Model_Comparison_Results.csv\", index=False)\n",
    "    print(\"Aggregated results saved to Model_Comparison_Results.csv\")\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = ModelPipeline(\n",
    "    model=LinearDiscriminantAnalysis(),\n",
    "    name=\"lda\",\n",
    "    standardize=True,\n",
    "    with_pca=False\n",
    ")\n",
    "lda_model.create_pipeline()\n",
    "lda_grid = [\n",
    "    {\"lda__solver\": [\"svd\"]},\n",
    "    {\"lda__solver\": [\"lsqr\"], \"lda__shrinkage\": [0.0, 0.1, 0.5, 1.0, \"auto\"]}\n",
    "]\n",
    "lda_model.tune(X_train, y_train, lda_grid)\n",
    "print(f\"Best parameters found for {lda_model.name}\")\n",
    "print(lda_model.best_params)\n",
    "lda_model.evaluate(X_train, X_test, y_train, y_test)\n",
    "print(f\"results of evaluating {lda_model.name}\")\n",
    "print(lda_model.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = lda_model.results[\"Confusion Matrix\"]\n",
    "labels = sorted(np.unique(y_test))\n",
    "plot_confusion_matrix(confusion, labels, title=f\"Confusion Matrix for {lda_model.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA with PCA (10 components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_pca_model = ModelPipeline(\n",
    "    model=LinearDiscriminantAnalysis(),\n",
    "    name=\"lda_pca\",\n",
    "    standardize=True,\n",
    "    with_pca=True,\n",
    "    n_pca_components=10\n",
    ")\n",
    "lda_pca_model.create_pipeline()\n",
    "lda_pca_model.set_params({\n",
    "    f\"lda_pca__{key.split('__')[1]}\": value for key, value in lda_model.best_params.items() if \"__\" in key\n",
    "})\n",
    "lda_pca_model.evaluate(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = lda_pca_model.results[\"Confusion Matrix\"]\n",
    "labels = sorted(np.unique(y_test))\n",
    "plot_confusion_matrix(confusion, labels, title=f\"Confusion Matrix for {lda_pca_model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate and Save Results\n",
    "results_df = aggregate_results(models)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Logistic with PCA 10 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# QDA Configuration\n",
    "qda_config = {\n",
    "    \"model\": QuadraticDiscriminantAnalysis(),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": False},\n",
    "    \"fixed_params\": {\"qda__priors\": None},\n",
    "    \"grid_search_params\": {\n",
    "        \"qda__reg_param\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune QDA\n",
    "qda_results = grid_search_tuning(\"qda\", qda_config, X_train, y_train)\n",
    "\n",
    "# Evaluate QDA\n",
    "qda_eval = evaluate_optimized_model(\"qda\", qda_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(qda_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning K-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# KNN Configuration\n",
    "knn_config = {\n",
    "    \"model\": KNeighborsClassifier(),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": False},\n",
    "    \"fixed_params\": {\n",
    "        \"knn__metric\": \"minkowski\",\n",
    "        \"knn__n_jobs\": 1\n",
    "    },\n",
    "    \"grid_search_params\": {\n",
    "        \"knn__p\": [0.8, 1.0, 1.2, 1.4, 1.7, 2.0, 2.2, 3.0, 10.0],\n",
    "        \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "        \"knn__n_neighbors\": [1, 3, 5, 9, 11, 15, 20, 30]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune KNN\n",
    "knn_results = grid_search_tuning(\"knn\", knn_config, X_train, y_train)\n",
    "\n",
    "# Evaluate KNN\n",
    "knn_eval = evaluate_optimized_model(\"knn\", knn_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(knn_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# SVM Configuration\n",
    "svm_config = {\n",
    "    \"model\": SVC(probability=True, random_state=42),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": False},\n",
    "    \"fixed_params\": {\"svm__degree\": 3},\n",
    "    \"grid_search_params\": {\n",
    "        \"svm__C\": [0.001, 0.101, 0.201, 0.301, 0.401, 0.501, 0.601, 0.701, 0.801, 0.901, 1.001, 5, 10],\n",
    "        \"svm__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "        \"svm__gamma\": [\"scale\", \"auto\"],\n",
    "        \"svm__class_weight\": [None, \"balanced\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune SVM\n",
    "svm_results = grid_search_tuning(\"svm\", svm_config, X_train, y_train)\n",
    "\n",
    "# Evaluate SVM\n",
    "svm_eval = evaluate_optimized_model(\"svm\", svm_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(svm_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# Random Forest Configuration\n",
    "rf_config = {\n",
    "    \"model\": RandomForestClassifier(random_state=42),\n",
    "    \"preprocess\": {\"standardize\": False, \"pca\": False},\n",
    "    \"fixed_params\": {\"rf__bootstrap\": True},\n",
    "    \"grid_search_params\": {\n",
    "        \"rf__n_estimators\": [100, 200, 300, 400],\n",
    "        \"rf__criterion\": [\"gini\", \"entropy\"],\n",
    "        \"rf__max_features\": [\"sqrt\", \"log2\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune Random Forest\n",
    "rf_results = grid_search_tuning(\"rf\", rf_config, X_train, y_train)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_eval = evaluate_optimized_model(\"rf\", rf_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(rf_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# Gradient Boosting Configuration\n",
    "gbdt_config = {\n",
    "    \"model\": GradientBoostingClassifier(random_state=42),\n",
    "    \"preprocess\": {\"standardize\": False, \"pca\": False},\n",
    "    \"fixed_params\": {\n",
    "        \"gbdt__loss\": \"log_loss\",\n",
    "        \"gbdt__n_iter_no_change\": 15\n",
    "    },\n",
    "    \"grid_search_params\": {\n",
    "        \"gbdt__n_estimators\": [100, 200, 300, 400, 500],\n",
    "        \"gbdt__max_depth\": [1, 3, 5, 7, 9],\n",
    "        \"gbdt__learning_rate\": [0.1, 0.05, 0.2, 0.3],\n",
    "        \"gbdt__max_features\": [\"sqrt\", 100, 130]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune Gradient Boosting\n",
    "gbdt_results = grid_search_tuning(\"gbdt\", gbdt_config, X_train, y_train)\n",
    "\n",
    "# Evaluate Gradient Boosting\n",
    "gbdt_eval = evaluate_optimized_model(\"gbdt\", gbdt_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(gbdt_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine evaluation results for all models without PCA (after completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all evaluation results into a single DataFrame\n",
    "final_results = pd.DataFrame([\n",
    "    lda_eval, \n",
    "    logit_eval, \n",
    "    qda_eval, \n",
    "    knn_eval, \n",
    "    svm_eval, \n",
    "    rf_eval, \n",
    "    gbdt_eval\n",
    "])\n",
    "\n",
    "# Save the combined results to a CSV\n",
    "final_results.to_csv(\"no_pca_results.csv\", index=False)\n",
    "\n",
    "# Display the final results\n",
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning stage with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# LDA Configuration with PCA\n",
    "lda_pca_config = {\n",
    "    \"model\": LinearDiscriminantAnalysis(),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": True},  # Enable PCA\n",
    "    \"fixed_params\": {\"lda_pca__tol\": 0.0001},\n",
    "    \"grid_search_params\": [\n",
    "        {\"lda_pca__solver\": [\"svd\"], \"pca__n_components\": [5, 10, 20]},\n",
    "        {\"lda_pca__solver\": [\"lsqr\"], \"lda_pca__shrinkage\": [0.0, 0.1, 0.5, \"auto\"], \"pca__n_components\": [5, 10, 20]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Tune LDA with PCA\n",
    "lda_pca_results = grid_search_tuning(\"lda_pca\", lda_pca_config, X_train, y_train)\n",
    "\n",
    "# Evaluate LDA with PCA\n",
    "lda_pca_eval = evaluate_optimized_model(\"lda_pca\", lda_pca_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "lda_pca_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# Logistic Regression Configuration with PCA\n",
    "logit_pca_config = {\n",
    "    \"model\": LogisticRegression(random_state=42),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": True},\n",
    "    \"fixed_params\": {\n",
    "        \"logit_pca__max_iter\": 100,\n",
    "        \"logit_pca__solver\": \"saga\",\n",
    "        \"logit_pca__n_jobs\": -1,\n",
    "        \"logit_pca__penalty\": \"elasticnet\"\n",
    "    },\n",
    "    \"grid_search_params\": {\n",
    "        \"logit_pca__C\": [0.01, 0.1, 1, 10],\n",
    "        \"logit_pca__l1_ratio\": [0.1, 0.5, 0.9],\n",
    "        \"logit_pca__class_weight\": [None, \"balanced\"],\n",
    "        \"pca__n_components\": [5, 10, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune Logistic Regression with PCA\n",
    "logit_pca_results = grid_search_tuning(\"logit_pca\", logit_pca_config, X_train, y_train)\n",
    "\n",
    "# Evaluate Logistic Regression with PCA\n",
    "logit_pca_eval = evaluate_optimized_model(\"logit_pca\", logit_pca_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "logit_pca_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QDA with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# QDA Configuration with PCA\n",
    "qda_pca_config = {\n",
    "    \"model\": QuadraticDiscriminantAnalysis(),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": True},\n",
    "    \"fixed_params\": {\"qda_pca__priors\": None},\n",
    "    \"grid_search_params\": {\n",
    "        \"qda_pca__reg_param\": [0.0, 0.1, 0.5, 1.0],\n",
    "        \"pca__n_components\": [5, 10, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Tune QDA with PCA\n",
    "qda_pca_results = grid_search_tuning(\"qda_pca\", qda_pca_config, X_train, y_train)\n",
    "\n",
    "# Evaluate QDA with PCA\n",
    "qda_pca_eval = evaluate_optimized_model(\"qda_pca\", qda_pca_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "qda_pca_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nn with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# KNN Configuration with PCA\n",
    "knn_pca_config = {\n",
    "    \"model\": KNeighborsClassifier(),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": True},\n",
    "    \"fixed_params\": {\n",
    "        \"knn_pca__metric\": \"minkowski\",\n",
    "        \"knn_pca__n_jobs\": 1\n",
    "    },\n",
    "    \"grid_search_params\": {\n",
    "        \"knn_pca__p\": [1, 2],  # Manhattan and Euclidean distances\n",
    "        \"knn_pca__weights\": [\"uniform\", \"distance\"],\n",
    "        \"knn_pca__n_neighbors\": [3, 5, 10],  # Common choices for neighbors\n",
    "        \"pca__n_components\": [5, 10, 20]  # Reduced dimensions\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune KNN with PCA\n",
    "knn_pca_results = grid_search_tuning(\"knn_pca\", knn_pca_config, X_train, y_train)\n",
    "\n",
    "# Evaluate KNN with PCA\n",
    "knn_pca_eval = evaluate_optimized_model(\"knn_pca\", knn_pca_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "knn_pca_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# SVM Configuration with PCA\n",
    "svm_pca_config = {\n",
    "    \"model\": SVC(probability=True, random_state=42),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": True},\n",
    "    \"fixed_params\": {\"svm_pca__degree\": 3},  # Use 'svm_pca__' to match pipeline step name\n",
    "    \"grid_search_params\": {\n",
    "        \"svm_pca__C\": [0.1, 1, 10],\n",
    "        \"svm_pca__kernel\": [\"linear\", \"rbf\"],\n",
    "        \"svm_pca__gamma\": [\"scale\", \"auto\"],\n",
    "        \"svm_pca__class_weight\": [None, \"balanced\"],\n",
    "        \"pca__n_components\": [5, 10, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune SVM with PCA\n",
    "svm_pca_results = grid_search_tuning(\"svm_pca\", svm_pca_config, X_train, y_train)\n",
    "\n",
    "# Evaluate SVM with PCA\n",
    "svm_pca_eval = evaluate_optimized_model(\"svm_pca\", svm_pca_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "svm_pca_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# Random Forest Configuration with PCA\n",
    "rf_pca_config = {\n",
    "    \"model\": RandomForestClassifier(random_state=42),\n",
    "    \"preprocess\": {\"standardize\": False, \"pca\": True},\n",
    "    \"fixed_params\": {\"rf_pca__bootstrap\": True},  # Correct parameter prefix\n",
    "    \"grid_search_params\": {\n",
    "        \"rf_pca__n_estimators\": [100, 200, 300],\n",
    "        \"rf_pca__criterion\": [\"gini\", \"entropy\"],\n",
    "        \"rf_pca__max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"pca__n_components\": [5, 10, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune Random Forest with PCA\n",
    "rf_pca_results = grid_search_tuning(\"rf_pca\", rf_pca_config, X_train, y_train)\n",
    "\n",
    "# Evaluate Random Forest with PCA\n",
    "rf_pca_eval = evaluate_optimized_model(\"rf_pca\", rf_pca_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(rf_pca_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# Gradient Boosting Configuration with PCA\n",
    "gbdt_pca_config = {\n",
    "    \"model\": GradientBoostingClassifier(random_state=42),\n",
    "    \"preprocess\": {\"standardize\": False, \"pca\": True},\n",
    "    \"fixed_params\": {\"gbdt_pca__loss\": \"log_loss\", \"gbdt_pca__n_iter_no_change\": 15},  # Correct parameter prefix\n",
    "    \"grid_search_params\": {\n",
    "        \"gbdt_pca__n_estimators\": [100, 200],\n",
    "        \"gbdt_pca__max_depth\": [3, 5],\n",
    "        \"gbdt_pca__learning_rate\": [0.05, 0.1, 0.2],\n",
    "        \"gbdt_pca__max_features\": [\"sqrt\", 100],\n",
    "        \"pca__n_components\": [5, 10, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune Gradient Boosting with PCA\n",
    "gbdt_pca_results = grid_search_tuning(\"gbdt_pca\", gbdt_pca_config, X_train, y_train)\n",
    "\n",
    "# Evaluate Gradient Boosting with PCA\n",
    "gbdt_pca_eval = evaluate_optimized_model(\"gbdt_pca\", gbdt_pca_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(gbdt_pca_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine PCA evaluation results into a single DataFrame\n",
    "pca_results = pd.DataFrame([\n",
    "    lda_pca_eval, \n",
    "    logit_pca_eval, \n",
    "    qda_pca_eval, \n",
    "    knn_pca_eval, \n",
    "    svm_pca_eval, \n",
    "    rf_pca_eval, \n",
    "    gbdt_pca_eval\n",
    "])\n",
    "\n",
    "# Save the PCA-only results to a CSV\n",
    "pca_results.to_csv(\"pca_results.csv\", index=False)\n",
    "\n",
    "# Display the PCA-only results\n",
    "print(pca_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining full results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all evaluation results (standard + PCA) into a single DataFrame\n",
    "final_results = pd.DataFrame([\n",
    "    lda_eval, \n",
    "    lda_pca_eval, \n",
    "    logit_eval, \n",
    "    logit_pca_eval, \n",
    "    qda_eval, \n",
    "    qda_pca_eval, \n",
    "    knn_eval, \n",
    "    knn_pca_eval, \n",
    "    svm_eval, \n",
    "    svm_pca_eval, \n",
    "    rf_eval, \n",
    "    rf_pca_eval, \n",
    "    gbdt_eval, \n",
    "    gbdt_pca_eval\n",
    "])\n",
    "\n",
    "# Save the full results to a CSV\n",
    "final_results.to_csv(\"final_results.csv\", index=False)\n",
    "\n",
    "# Display the full results\n",
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost Without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Configuration without PCA\n",
    "ada_boost_no_pca_config = {\n",
    "    \"model\": AdaBoostClassifier(random_state=42),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": False},\n",
    "    \"fixed_params\": {\n",
    "        \"ada_boost_no_pca__algorithm\": \"SAMME\"  # Correct prefix for pipeline step\n",
    "    },\n",
    "    \"grid_search_params\": {\n",
    "        \"ada_boost_no_pca__estimator\": [DecisionTreeClassifier(max_depth=2), LogisticRegression()],  # Use 'estimator' instead of 'base_estimator'\n",
    "        \"ada_boost_no_pca__n_estimators\": [50, 100, 200],\n",
    "        \"ada_boost_no_pca__learning_rate\": [0.05, 0.1, 0.5, 1]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune AdaBoost without PCA\n",
    "ada_boost_no_pca_results, ada_boost_no_pca_best_params = grid_search_tuning(\"ada_boost_no_pca\", ada_boost_no_pca_config, X_train, y_train)\n",
    "\n",
    "# Evaluate AdaBoost without PCA\n",
    "ada_boost_no_pca_eval = evaluate_optimized_model(\"ada_boost_no_pca\", ada_boost_no_pca_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Print Evaluation Results\n",
    "print(ada_boost_no_pca_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# AdaBoost Configuration with PCA\n",
    "ada_boost_pca_config = {\n",
    "    \"model\": AdaBoostClassifier(random_state=42),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": True},  # Enable PCA\n",
    "    \"fixed_params\": {\n",
    "        \"model__algorithm\": \"SAMME\"  # Correct prefix for pipeline step\n",
    "    },\n",
    "    \"grid_search_params\": {\n",
    "        \"model__estimator\": [DecisionTreeClassifier(max_depth=2, random_state=42), LogisticRegression(random_state=42)],\n",
    "        \"model__n_estimators\": [50, 100, 200],\n",
    "        \"model__learning_rate\": [0.05, 0.1, 0.5, 1],\n",
    "        \"pca__n_components\": [5, 10, 20]  # PCA dimensionality reduction\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune AdaBoost with PCA\n",
    "ada_boost_pca_results, ada_boost_pca_best_params = grid_search_tuning(\"ada_boost_pca\", ada_boost_pca_config, X_train, y_train)\n",
    "\n",
    "# Evaluate AdaBoost with PCA\n",
    "ada_boost_pca_eval = evaluate_optimized_model(\"ada_boost_pca\", ada_boost_pca_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Print Evaluation Results\n",
    "print(ada_boost_pca_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting classifier without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# Voting Classifier without PCA\n",
    "voting_no_pca_config = {\n",
    "    \"model\": VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(random_state=42)),\n",
    "            ('svm', SVC(probability=True, random_state=42)),\n",
    "            ('rf', RandomForestClassifier(random_state=42))\n",
    "        ]\n",
    "    ),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": False},  # PCA disabled\n",
    "    \"fixed_params\": {},  # No fixed parameters specific to voting\n",
    "    \"grid_search_params\": {\n",
    "        \"voting_no_pca__voting\": [\"hard\", \"soft\"]  # Prefix with the pipeline step name\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune Voting Classifier without PCA\n",
    "voting_no_pca_results = grid_search_tuning(\"voting_no_pca\", voting_no_pca_config, X_train, y_train)\n",
    "\n",
    "# Evaluate Voting Classifier without PCA\n",
    "voting_no_pca_eval = evaluate_optimized_model(\"voting_no_pca\", voting_no_pca_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Print Results\n",
    "print(voting_no_pca_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting classifier with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# Voting Classifier with PCA\n",
    "voting_pca_config = {\n",
    "    \"model\": VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(random_state=42)),\n",
    "            ('svm', SVC(probability=True, random_state=42)),\n",
    "            ('rf', RandomForestClassifier(random_state=42))\n",
    "        ]\n",
    "    ),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": True},  # PCA enabled\n",
    "    \"fixed_params\": {},  # No fixed parameters specific to voting\n",
    "    \"grid_search_params\": {\n",
    "        \"voting_pca__voting\": [\"hard\", \"soft\"],  # Prefix to match pipeline step\n",
    "        \"pca__n_components\": [5, 10, 20]  # Parameter for PCA\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune Voting Classifier with PCA\n",
    "voting_pca_results = grid_search_tuning(\"voting_pca\", voting_pca_config, X_train, y_train)\n",
    "\n",
    "# Evaluate Voting Classifier with PCA\n",
    "voting_pca_eval = evaluate_optimized_model(\"voting_pca\", voting_pca_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Print Evaluation Results\n",
    "print(voting_pca_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging classifiers without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# Bagging Classifier Configuration\n",
    "bagging_config = {\n",
    "    \"model\": BaggingClassifier(random_state=42),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": False},  # PCA disabled\n",
    "    \"fixed_params\": {\"bagging__n_jobs\": -1},  # Use all processors\n",
    "    \"grid_search_params\": {\n",
    "        \"bagging__n_estimators\": [10, 50, 100],\n",
    "        \"bagging__estimator\": [DecisionTreeClassifier(max_depth=5), LogisticRegression()]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune Bagging Classifier\n",
    "bagging_results = grid_search_tuning(\"bagging\", bagging_config, X_train, y_train)\n",
    "\n",
    "# Evaluate Bagging Classifier\n",
    "bagging_eval = evaluate_optimized_model(\"bagging\", bagging_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Print Evaluation Results\n",
    "print(bagging_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet Logistic Regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# ElasticNet Logistic Regression with PCA Configuration\n",
    "elasticnet_pca_config = {\n",
    "    \"model\": LogisticRegression(penalty=\"elasticnet\", solver=\"saga\", random_state=42),\n",
    "    \"preprocess\": {\"standardize\": True, \"pca\": True},  # PCA enabled\n",
    "    \"fixed_params\": {\"elasticnet_logit_pca__max_iter\": 500},  # Correct prefix for pipeline step\n",
    "    \"grid_search_params\": {\n",
    "        \"elasticnet_logit_pca__C\": [0.01, 0.1, 1, 10],  # Correct prefix for pipeline step\n",
    "        \"elasticnet_logit_pca__l1_ratio\": [0.1, 0.5, 0.9],  # Correct prefix for pipeline step\n",
    "        \"pca__n_components\": [10, 25, 50]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune ElasticNet Logistic Regression with PCA\n",
    "elasticnet_pca_results = grid_search_tuning(\"elasticnet_logit_pca\", elasticnet_pca_config, X_train, y_train)\n",
    "\n",
    "# Evaluate ElasticNet Logistic Regression with PCA\n",
    "elasticnet_pca_eval = evaluate_optimized_model(\"elasticnet_logit_pca\", elasticnet_pca_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "elasticnet_pca_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Evaluation Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StackingClassifier with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = \"Datasets/Data 1.csv.gz\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_split(data, label_col=\"label\", random_state=42, test_size=0.2)\n",
    "\n",
    "# Iterate over final estimators for stacking\n",
    "final_estimators = [\n",
    "    LogisticRegression(max_iter=500, random_state=42),\n",
    "    GradientBoostingClassifier(random_state=42)\n",
    "]\n",
    "\n",
    "stacking_results = []\n",
    "\n",
    "for estimator in final_estimators:\n",
    "    # Stacking Classifier with PCA Configuration\n",
    "    stacking_pca_config = {\n",
    "        \"model\": StackingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(random_state=42)),\n",
    "                ('rf', RandomForestClassifier(random_state=42)),\n",
    "                ('svc', SVC(probability=True, random_state=42))\n",
    "            ],\n",
    "            final_estimator=estimator\n",
    "        ),\n",
    "        \"preprocess\": {\"standardize\": True, \"pca\": True},  # PCA enabled\n",
    "        \"fixed_params\": {},  # No fixed parameters specific to stacking\n",
    "        \"grid_search_params\": {\n",
    "            \"pca__n_components\": [10, 25, 50]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Tune Stacking Classifier with PCA\n",
    "    stacking_pca_results = grid_search_tuning(f\"stacking_pca_{estimator.__class__.__name__.lower()}\", stacking_pca_config, X_train, y_train)\n",
    "\n",
    "    # Evaluate Stacking Classifier with PCA\n",
    "    stacking_pca_eval = evaluate_optimized_model(f\"stacking_pca_{estimator.__class__.__name__.lower()}\", stacking_pca_config, \"Tuning_params\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Append results\n",
    "    stacking_results.append(stacking_pca_eval)\n",
    "\n",
    "# Print Evaluation Results\n",
    "for result in stacking_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all evaluation results into a single DataFrame\n",
    "final_results = pd.DataFrame([\n",
    "    ada_boost_no_pca_eval,\n",
    "    ada_boost_pca_eval,     \n",
    "    voting_no_pca_eval,      \n",
    "    voting_pca_eval,         \n",
    "    bagging_eval,             \n",
    "    elasticnet_pca_eval,     \n",
    "    stacking_pca_eval         \n",
    "])\n",
    "\n",
    "# Save the combined results to a CSV\n",
    "final_results.to_csv(\"final_results_all_models.csv\", index=False)\n",
    "\n",
    "# Display the final results\n",
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mypredict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ST443",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
