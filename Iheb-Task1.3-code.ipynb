{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Models import\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(model, model_name, standardize=False, with_pca=False, n_pca_components=10):\n",
    "    \"\"\"\n",
    "    Creates a pipeline for a given model with optional standardization and PCA.\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    if standardize:\n",
    "        steps.append(('scaler', StandardScaler()))\n",
    "    if with_pca:\n",
    "        steps.append(('pca', PCA(n_components=n_pca_components)))\n",
    "    steps.append((model_name, model))\n",
    "    return Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_tuning(model_name, config, X_train, y_train, scoring=\"f1\", cv_folds=5, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for a model pipeline using GridSearchCV.\n",
    "\n",
    "    This function creates a pipeline with preprocessing and the model, applies \n",
    "    hyperparameter tuning using GridSearchCV, and saves the results.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): Name of the model for identification in outputs.\n",
    "    - config (dict): Contains the model, preprocessing options, fixed params, \n",
    "                     and hyperparameter grid.\n",
    "    - X_train, y_train: Training data and labels.\n",
    "    - scoring (str): Metric to optimize (default: \"f1\").\n",
    "    - cv_folds (int): Number of cross-validation folds (default: 5).\n",
    "    - n_jobs (int): Number of parallel jobs (-1 uses all cores).\n",
    "\n",
    "    Returns:\n",
    "    - grid_search (GridSearchCV): Fitted GridSearchCV object.\n",
    "    - best_params (dict): Best hyperparameters and metadata (e.g., time taken).\n",
    "\n",
    "    Saves:\n",
    "    - Full tuning results to CSV.\n",
    "    - Best hyperparameters to JSON.\n",
    "    \"\"\"\n",
    "    pipeline = create_pipeline(\n",
    "        model=config[\"model\"],\n",
    "        model_name=model_name,\n",
    "        standardize=config[\"preprocess\"].get(\"standardize\", False),\n",
    "        with_pca=config[\"preprocess\"].get(\"pca\", False),\n",
    "        n_pca_components=config[\"preprocess\"].get(\"pca_components\", 10)\n",
    "    )\n",
    "    pipeline.set_params(**config[\"fixed_params\"])\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid=config[\"grid_search_params\"],\n",
    "        scoring=scoring,\n",
    "        cv=cv_folds,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=2\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    elapsed_time = (time.time() - start_time) / 60\n",
    "    \n",
    "    best_params = grid_search.best_params_\n",
    "    best_params[\"MODEL_NAME\"] = model_name\n",
    "    best_params[\"TIME_ELAPSED_MIN\"] = elapsed_time\n",
    "\n",
    "    # Save results\n",
    "    pd.DataFrame(grid_search.cv_results_).to_csv(f\"Tuning_params/{model_name}_results.csv\", index=False)\n",
    "    with open(f\"Tuning_params/{model_name}_best_params.json\", \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "\n",
    "    print(f\"Model {model_name} tuned in {elapsed_time:.2f} minutes.\")\n",
    "    return grid_search, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_optimized_models(results_dir, models_config, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates optimized models on the test set using the best parameters from tuning.\n",
    "\n",
    "    This function loads the best hyperparameters saved during grid search for each model, \n",
    "    reconstructs the pipeline with the optimal parameters, and evaluates its performance \n",
    "    on the test data. Results include classification metrics, confusion matrices, and AUC scores.\n",
    "\n",
    "    Parameters:\n",
    "    - results_dir (str): Directory containing JSON files with the best parameters for each model.\n",
    "    - models_config (dict): Configuration dictionary for the models, including model instances \n",
    "                            and preprocessing settings (e.g., PCA, standardization).\n",
    "    - X_test (array-like): Features of the test dataset.\n",
    "    - y_test (array-like): True labels of the test dataset.\n",
    "\n",
    "    Returns:\n",
    "    - results (list): A list of dictionaries, one for each model, containing:\n",
    "        - \"model_name\": Name of the evaluated model.\n",
    "        - \"classification_report\": Detailed classification metrics (precision, recall, F1, etc.).\n",
    "        - \"auc_score\": AUC score if the model supports `predict_proba`; otherwise, None.\n",
    "        - \"confusion_matrix\": Confusion matrix for the model's predictions.\n",
    "        - \"error\" (if any): Error message for models that failed during evaluation.\n",
    "\n",
    "    Workflow:\n",
    "    1. For each model in `models_config`, load its best parameters from the JSON file.\n",
    "    2. Recreate the pipeline with preprocessing steps (e.g., PCA, standardization) and the model.\n",
    "    3. Set the pipeline's parameters to the best hyperparameters found during tuning.\n",
    "    4. Evaluate the pipeline on the test dataset and compute:\n",
    "       - Classification report with metrics like precision, recall, F1-score.\n",
    "       - Confusion matrix.\n",
    "       - AUC score (if applicable).\n",
    "    5. Append the results to a list, including any errors encountered during the process.\n",
    "\n",
    "    Example Output:\n",
    "    [\n",
    "        {\n",
    "            \"model_name\": \"LogisticRegression\",\n",
    "            \"classification_report\": {...},\n",
    "            \"auc_score\": 0.92,\n",
    "            \"confusion_matrix\": [[85, 15], [8, 112]],\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"LDA\",\n",
    "            \"error\": \"Pipeline fitting failed due to incompatible parameter grid.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    Side Effects:\n",
    "    - Prints progress for each model evaluation.\n",
    "    - Logs any errors encountered during evaluation.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for model_name, config in models_config.items():\n",
    "        try:\n",
    "            # Load best parameters\n",
    "            with open(f\"{results_dir}/{model_name}_best_params.json\", \"r\") as f:\n",
    "                best_params = json.load(f)\n",
    "            \n",
    "            # Create pipeline\n",
    "            pipeline = create_pipeline(\n",
    "                model=config[\"model\"],\n",
    "                model_name=model_name,\n",
    "                standardize=config[\"preprocess\"].get(\"standardize\", False),\n",
    "                with_pca=config[\"preprocess\"].get(\"pca\", False),\n",
    "                n_pca_components=config[\"preprocess\"].get(\"pca_components\", 10)\n",
    "            )\n",
    "            pipeline.set_params(**best_params)\n",
    "\n",
    "            # Evaluate\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            metrics = classification_report(y_test, y_pred, output_dict=True)\n",
    "            auc_score = roc_auc_score(y_test, pipeline.predict_proba(X_test)[:, 1]) if hasattr(pipeline, \"predict_proba\") else None\n",
    "            confusion = confusion_matrix(y_test, y_pred).tolist()\n",
    "\n",
    "            results.append({\n",
    "                \"model_name\": model_name,\n",
    "                \"classification_report\": metrics,\n",
    "                \"auc_score\": auc_score,\n",
    "                \"confusion_matrix\": confusion,\n",
    "            })\n",
    "            print(f\"Evaluated model: {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error with model {model_name}: {e}\")\n",
    "            results.append({\"model_name\": model_name, \"error\": str(e)})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mypredict():\n",
    "    \"\"\"\n",
    "    Reads test.csv.gz, predicts class labels using the best model, \n",
    "    and writes the predictions to predictions.txt.\n",
    "    \"\"\"\n",
    "    test_data = pd.read_csv(\"test.csv.gz\")\n",
    "    X_test = test_data.iloc[:, 1:].values  # Exclude label column\n",
    "    model_name = \"LogisticRegression\"  # Example; replace with your best model\n",
    "    with open(f\"Tuning_params/{model_name}_best_params.json\", \"r\") as f:\n",
    "        best_params = json.load(f)\n",
    "\n",
    "    # Configure pipeline\n",
    "    pipeline = create_pipeline(\n",
    "        model=LogisticRegression(),  # Replace with our chosen model\n",
    "        model_name=model_name,\n",
    "        standardize=True,\n",
    "        with_pca=False\n",
    "    )\n",
    "    pipeline.set_params(**best_params)\n",
    "\n",
    "    # Predict and save\n",
    "    predictions = pipeline.predict(X_test)\n",
    "    with open(\"predictions.txt\", \"w\") as f:\n",
    "        f.writelines(f\"{label}\\n\" for label in predictions)\n",
    "    print(\"Predictions saved to predictions.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")  # Replace with your training dataset path\n",
    "\n",
    "# Prepare features and labels\n",
    "label_col = \"label\"  # Replace with your actual label column name\n",
    "X = data.drop(columns=[label_col]).values\n",
    "y = LabelEncoder().fit_transform(data[label_col].values)\n",
    "\n",
    "# Split into training and testing sets\n",
    "random_state = 42\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary for all the models you want to tune and evaluate, based on your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_config = {\n",
    "    \"LDA\": {\n",
    "        \"model\": LinearDiscriminantAnalysis(),\n",
    "        \"preprocess\": {\"standardize\": True, \"pca\": False},\n",
    "        \"fixed_params\": {\"lda__tol\": 0.0001},\n",
    "        \"grid_search_params\": [\n",
    "            {\"lda__solver\": [\"svd\"]},\n",
    "            {\"lda__solver\": [\"lsqr\"], \"lda__shrinkage\": [0.0, 0.1, 0.5, \"auto\"]},\n",
    "        ],\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"preprocess\": {\"standardize\": True, \"pca\": False},\n",
    "        \"fixed_params\": {\"logit__max_iter\": 100, \"logit__solver\": \"saga\"},\n",
    "        \"grid_search_params\": {\n",
    "            \"logit__C\": [0.01, 0.1, 1, 10],\n",
    "            \"logit__penalty\": [\"l2\", \"elasticnet\"],\n",
    "            \"logit__l1_ratio\": [0.1, 0.5, 0.9],\n",
    "        },\n",
    "    },\n",
    "    # Add other models similarly...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the grid_search_tuning function to tune hyperparameters for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Ensure the directory for saving results exists\n",
    "Path(\"Tuning_params\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Tune each model\n",
    "for model_name, config in models_config.items():\n",
    "    grid_search, best_params = grid_search_tuning(\n",
    "        model_name=model_name,\n",
    "        config=config,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        scoring=\"f1\",  # Optimize for F1 score\n",
    "        cv_folds=5,    # 5-fold cross-validation\n",
    "        n_jobs=-1      # Use all available cores\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate optimized models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once tuning is complete, evaluate the best models on the test set using evaluate_optimized_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_optimized_models(\n",
    "    results_dir=\"Tuning_params\",  # Directory containing best parameter JSON files\n",
    "    models_config=models_config,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Print evaluation results\n",
    "for result in results:\n",
    "    print(f\"Model: {result['model_name']}\")\n",
    "    if \"error\" in result:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    else:\n",
    "        print(\"Classification Report:\")\n",
    "        print(result[\"classification_report\"])\n",
    "        print(\"AUC Score:\", result[\"auc_score\"])\n",
    "        print(\"Confusion Matrix:\", result[\"confusion_matrix\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions for a test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the mypredict function to predict and save labels for a test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Output \n",
    "1. Hyperparameter Tuning:\n",
    "- JSON files containing the best parameters for each model (e.g., Tuning_params/LDA_best_params.json).\n",
    "- CSV files with detailed cross-validation results for each model.\n",
    "\n",
    "2. Evaluation:\n",
    "- Printed classification reports, confusion matrices, and AUC scores.\n",
    "- Results as a list of dictionaries.\n",
    "\n",
    "3. Predictions:\n",
    "- A text file (predictions.txt) containing one label per line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ST443",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
