{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import json # Export dictionary of tuned parameters.\n",
    "from sklearn.model_selection import train_test_split, TunedThresholdClassifierCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV # For hyperparameter tuning.\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation metrics import\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, \n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Models import\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Visualisations import\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for data, which is most important split.\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Setup for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path, compression='gzip')\n",
    "\n",
    "# Train Data\n",
    "def train_data(data, model, label_col, test_size=0.2, random_state=42, standardize=True, with_pca=False, n_pca_components=10):\n",
    "    \"\"\"\n",
    "    Preprocess data and gives back fitted model. Includes different options mainly related to standardization of features and use of PCA.\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: {sklearn object} Non fitted model object from sklearn. Should be tuned already.\n",
    "        \n",
    "    TO DO\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    TO DO    \n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    # Split the dataset into features and labels\n",
    "    X = data.drop(columns=[label_col]).values\n",
    "    y = LabelEncoder().fit_transform(data[label_col].values)\n",
    "\n",
    "    # Split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Standardize features (fit on training set, transform both train and test if necessary)\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Apply PCA (fit on training set, transform both train and test if necessary)\n",
    "    if with_pca:\n",
    "        pca = PCA(n_components=n_pca_components)\n",
    "        X_train = pca.fit_transform(X_train)\n",
    "        X_test = pca.transform(X_test) \n",
    "\n",
    "    # Train the model\n",
    "    print(f\"Training {model.__class__.__name__}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model, X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Evaluate Model\n",
    "def evaluate(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Takes in any fitted model and gives all the metrics that are asked\n",
    "    Should be used for the results of the hypertuned model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: {sklearn object} Fitted model from sklearn. \n",
    "    \n",
    "    X_test: {array like n*p} Features of test set/left out sample.\n",
    "\n",
    "    y_test: {array like n*1} Feature that is being predicted. \n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    results_df: {pandas df} Contains all metrics\n",
    "\n",
    "    confusion: Elements to plot confusion matrix.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Prepare metrics for DataFrame\n",
    "    results = {\n",
    "        \"Metric\": [\"Accuracy\", \"Balanced Accuracy\", \"F1 Score\", \"AUC\"],\n",
    "        \"Value\": [accuracy, balanced_accuracy, f1, auc_score]\n",
    "    }\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df, confusion\n",
    "\n",
    "def cross_validate(model,X_train,y_train):\n",
    "    \"\"\"\n",
    "    Check how to build the hyperparameter tuning, working below.\n",
    "    Employ cross validation AND Hypertune to maximize F1 score, that way we kill 2 birds in one stone.\n",
    "    \"\"\"\n",
    "\n",
    "    # Should return the optimal attributes for t\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion, labels, title=\"Confusion Matrix\"):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow with a loop (Looping through the classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers (with standardize parameter)\n",
    "\n",
    "This dictionary here can be modified. Within each model we add the tuning parameters from the cross validation.\n",
    "But then we don't have to change anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"LDA\": {\n",
    "        \"model\": LinearDiscriminantAnalysis(),\n",
    "        \"standardize\": True\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(max_iter=1000),\n",
    "        \"standardize\": True\n",
    "    },\n",
    "    \"QDA\": {\n",
    "        \"model\": QuadraticDiscriminantAnalysis(),\n",
    "        \"standardize\": True\n",
    "    },\n",
    "    \"k-NN\": {\n",
    "        \"model\": KNeighborsClassifier(n_neighbors=5),\n",
    "        \"standardize\": True\n",
    "    },\n",
    "    \"GBDT\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"standardize\": False\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"standardize\": False\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(probability=True),\n",
    "        \"standardize\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data from the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Datasets/data1.csv.gz\" \n",
    "label_column = \"label\" \n",
    "\n",
    "# Load the dataset\n",
    "data = load_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>With PCA</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDA</td>\n",
       "      <td>False</td>\n",
       "      <td>0.700457</td>\n",
       "      <td>0.700674</td>\n",
       "      <td>0.632287</td>\n",
       "      <td>0.738854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>False</td>\n",
       "      <td>0.953425</td>\n",
       "      <td>0.950146</td>\n",
       "      <td>0.936646</td>\n",
       "      <td>0.991995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QDA</td>\n",
       "      <td>False</td>\n",
       "      <td>0.528767</td>\n",
       "      <td>0.539963</td>\n",
       "      <td>0.475610</td>\n",
       "      <td>0.539963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>k-NN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.729680</td>\n",
       "      <td>0.666834</td>\n",
       "      <td>0.538941</td>\n",
       "      <td>0.714191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GBDT</td>\n",
       "      <td>False</td>\n",
       "      <td>0.948858</td>\n",
       "      <td>0.937660</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.989834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>False</td>\n",
       "      <td>0.941553</td>\n",
       "      <td>0.923532</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.989892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVM</td>\n",
       "      <td>False</td>\n",
       "      <td>0.964384</td>\n",
       "      <td>0.953582</td>\n",
       "      <td>0.949547</td>\n",
       "      <td>0.992598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  With PCA  Accuracy  Balanced Accuracy  F1 Score  \\\n",
       "0                   LDA     False  0.700457           0.700674  0.632287   \n",
       "2   Logistic Regression     False  0.953425           0.950146  0.936646   \n",
       "4                   QDA     False  0.528767           0.539963  0.475610   \n",
       "6                  k-NN     False  0.729680           0.666834  0.538941   \n",
       "8                  GBDT     False  0.948858           0.937660  0.927835   \n",
       "10        Random Forest     False  0.941553           0.923532  0.914894   \n",
       "12                  SVM     False  0.964384           0.953582  0.949547   \n",
       "\n",
       "         AUC  \n",
       "0   0.738854  \n",
       "2   0.991995  \n",
       "4   0.539963  \n",
       "6   0.714191  \n",
       "8   0.989834  \n",
       "10  0.989892  \n",
       "12  0.992598  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[results_df[\"With PCA\"]==False, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>With PCA</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDA</td>\n",
       "      <td>True</td>\n",
       "      <td>0.943379</td>\n",
       "      <td>0.924452</td>\n",
       "      <td>0.917112</td>\n",
       "      <td>0.989838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>True</td>\n",
       "      <td>0.960731</td>\n",
       "      <td>0.954874</td>\n",
       "      <td>0.945776</td>\n",
       "      <td>0.991586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>QDA</td>\n",
       "      <td>True</td>\n",
       "      <td>0.953425</td>\n",
       "      <td>0.945446</td>\n",
       "      <td>0.935197</td>\n",
       "      <td>0.989680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>k-NN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.931507</td>\n",
       "      <td>0.914551</td>\n",
       "      <td>0.901186</td>\n",
       "      <td>0.971316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GBDT</td>\n",
       "      <td>True</td>\n",
       "      <td>0.937900</td>\n",
       "      <td>0.926913</td>\n",
       "      <td>0.912821</td>\n",
       "      <td>0.984396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>True</td>\n",
       "      <td>0.944292</td>\n",
       "      <td>0.933530</td>\n",
       "      <td>0.921694</td>\n",
       "      <td>0.981991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SVM</td>\n",
       "      <td>True</td>\n",
       "      <td>0.960731</td>\n",
       "      <td>0.952785</td>\n",
       "      <td>0.945223</td>\n",
       "      <td>0.992376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  With PCA  Accuracy  Balanced Accuracy  F1 Score  \\\n",
       "1                   LDA      True  0.943379           0.924452  0.917112   \n",
       "3   Logistic Regression      True  0.960731           0.954874  0.945776   \n",
       "5                   QDA      True  0.953425           0.945446  0.935197   \n",
       "7                  k-NN      True  0.931507           0.914551  0.901186   \n",
       "9                  GBDT      True  0.937900           0.926913  0.912821   \n",
       "11        Random Forest      True  0.944292           0.933530  0.921694   \n",
       "13                  SVM      True  0.960731           0.952785  0.945223   \n",
       "\n",
       "         AUC  \n",
       "1   0.989838  \n",
       "3   0.991586  \n",
       "5   0.989680  \n",
       "7   0.971316  \n",
       "9   0.984396  \n",
       "11  0.981991  \n",
       "13  0.992376  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[results_df[\"With PCA\"]==True, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning. \n",
    "Tune the hyper parameters with the goal of improving the <b>F1 score</b>. \n",
    "\n",
    "Tough to completely merge with other part of the code. Ideally we get the results in a dict and then don't have to rerun that code ever again.\n",
    "\n",
    "A search consists of:\n",
    "an estimator (regressor or classifier such as sklearn.svm.SVC()) <b>All our estimators are here, 7*2 for PCA</b>\n",
    "\n",
    "a parameter space <b>This is a bit of guess work</b>\n",
    "\n",
    "a method for searching or sampling candidates <b>Grid search CV</b>\n",
    "\n",
    "a cross-validation scheme; <b>5-fold</b>\n",
    "\n",
    "a score function.<b>F1 score</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideally save grid search results to a json file and then return to dict.\n",
    "\n",
    "- Unpack the dictionary into the model!! ** super easy!\n",
    "- Take care of the fixed params as well! Most of them have default values but logit for example has some fixed params which are not default, these are not saved. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing before each fit: using a pipeline!\n",
    "\n",
    "https://stackoverflow.com/questions/55692298/preprocessing-on-gridsearchcv\n",
    "\n",
    "https://www.youtube.com/watch?v=tIO8zPCdi58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens if a model from the cv takes too long?\n",
    "Break the model or what? Set limit to iter?\n",
    "\n",
    "- Gridsearch is pretty strong so even if inner model raises error it will not terminate, which is nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some paremeters need specific solver:\n",
    "- Solution is to give 2 dictionaries, so GridSearch will search over 2 grids. LDA will give an example.\n",
    "    https://scikit-learn.org/stable/modules/grid_search.html#exhaustive-grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling randomness -> The instructions want reproducibility.\n",
    "https://scikit-learn.org/stable/common_pitfalls.html#controlling-randomness\n",
    "- Recommendation is to set seed for every random estimator!!! Oh jesus.\n",
    "- Most of our estimators are non random (at least in theory).\n",
    "- What is clearly random is our CV in grid search.\n",
    "- Should fix the randomness of our estimators at least for the  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split(data,label_col,random_state,test_size=0.2):\n",
    "    \"\"\"\n",
    "    Needs to be like this now as we can't preprocess the train data as we will run cross validation on it, therefore we need to preprocess\n",
    "    on each split.\n",
    "\n",
    "    \"\"\"\n",
    "    X = data.drop(columns=[label_col]).values\n",
    "    y = LabelEncoder().fit_transform(data[label_col].values)\n",
    "    \n",
    "        # Split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_G, y_TRAIN_G, X_TEST_G ,y_TEST_G  = train_split(data,label_col=\"label\",random_state=RANDOM_STATE,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4376, 4123) (4376,) (1095, 4123) (1095,)\n"
     ]
    }
   ],
   "source": [
    "print(X_TRAIN_G.shape, y_TRAIN_G.shape, X_TEST_G.shape, y_TEST_G.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1095, 4123)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_TRAIN_G.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_maker(model_name, model,standardize=False,with_pca=False, n_pca_components=10):\n",
    "    \"\"\"\n",
    "    Makes different pipelines according to learning model and standarization/pca requirements\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    set_instructions = []\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        set_instructions.append((\"scaler\",scaler))\n",
    "    \n",
    "    # Apply PCA (fit on training set, transform both train and test if necessary)\n",
    "    if with_pca:\n",
    "        pca = PCA(n_components=n_pca_components)\n",
    "        set_instructions.append((\"pca\",pca))\n",
    "    set_instructions.append((model_name,model))    \n",
    "\n",
    "    pipe = Pipeline(set_instructions)\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The way the file is defined it will not overwrite, which is good, but should be taken into account.\n",
    "def hyper_param(path=\".json\",params={}):\n",
    "    with open(path, \"w\") as outfile: \n",
    "        json.dump(params, outfile)\n",
    "        outfile.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning params table:\n",
    "\n",
    "- The fixed params and grid search params all follow the \"model_name\"__\"param to modify\" convention, it could be automated\n",
    "but I think it is not needed. TO BE DISCUSSED.\n",
    "- If we have time I would like to run some basic data analysis on the cross fiting results, not just keeping the maximum.\n",
    "- Even if results are same as unparametrized, I am using less data to estimate the model so hopefully when we do the final fit, and check on the test set the results should improve.\n",
    "\n",
    "| Model | PCA | Standardization | Tuning params | Explanation | Preliminary F1       |   TIME |\n",
    "|----------------|--------------|------------|---|----|-----|--------|\n",
    "| Linear discriminant analysis  |  NO     | YES  |solver, shrinkage| Big covariance matrix reguralizing it will make it easier to estimate. <br /> Use the other solver because it supports shrinkage, svd just to compare to baseline we ran |    0.93 <b>Very nice now:)</b> | 15 min\n",
    "|Logistic regression |  NO | YES | C, l_ratio, class_weight| Mostly just use shrinkage, elastic nest nesting lasso and ridge, different regularisation strengths. Most interesting is class weights, have some balance.   |  0.94   | 87 min, no convergence |\n",
    "|QDA|NO|YES|some regularisation that is not working|| garbage|4 min|\n",
    "|K-nn|NO|YES|n_neighbors,metric,p,weights,n_jobs|Number of neigh is self explanatory, for distance minkowski is default but nests euclidenan and manhattan, testing a few distances nested in Minkowski. Weights is for decision rule of voting, closest in neighborhood have more weight or uniform|1000min |0.74 good improvement, room for more.|\n",
    "|SVM|NO|YES|C,kernel,gamma,class_weight|Kernels are the most interesting thing to tune, gamma is hyper param of some kernels, C is the regularisation that was important in some models, class weight I still don't know why I touch that|0.94 similar to unoptimized |129 min| \n",
    "|Random Forest|NO|NO|n_trees,criterion,max_features|Number of trees is key, up to 300, if it doesn't take an eternity I can go to 500. Criterion just because we showed in class. Features because it is interesting to see (log2 harsher). Finally no pruning, would take an eternity and random forest supposedly doesn't need it. Boot and max features should take care of overfitting.|0.91 same as unparametrized.|3 minutes, so room for more tuning then|\n",
    "|Gradient Boosted| NO  | NO |loss, learning_rate, max_depth, max_features, n_iter_no_change, n_estimators | Ok, I do not fully know what I am doing with this one. Learning rate is v from the book I think, it is a regularisation parameter. n estimators is the number of steps M, there is a tradeoff between M and v. Max depth is J from the book I think. Features introduces some random forest component to this algorithm. Finally early stopping to 15 because I am impatient.|0.938 small improvement but ok. |   87min |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# empty_dict ={\"key\":{\"hey\":\"2\"}}\n",
    "# if hasattr(empty_dict,\"key\"):\n",
    "#     print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tuning_lda = {\n",
    "    \"lda\": {\n",
    "        # IF PCA TRUE THE NAME SHOULD BE lda_pca\n",
    "        \"model\": LinearDiscriminantAnalysis(),\n",
    "        \"preprocess\": {\n",
    "            \"standardize\":True,\n",
    "            \"pca\":False\n",
    "        },\n",
    "        \"fixed_params\" : {\n",
    "            #No default parameters, but doing so would raise error, add fake one\n",
    "            \"lda__tol\":0.0001\n",
    "            },\n",
    "        # Two grids, one of the solvers does not support shrinkage\n",
    "        \"grid_search_params\":[\n",
    "            {\n",
    "            \"lda__solver\":[\"svd\"]\n",
    "        },\n",
    "            {\n",
    "            \"lda__solver\":[\"lsqr\"],\n",
    "            \"lda__shrinkage\":[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \"auto\"]   \n",
    "        }\n",
    "                             ]\n",
    "    \n",
    "    \n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i/10  for i in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# MODELS SHOULD BE NAMED model model_pca\n",
    "# Seems like it plateaus at 0.94\n",
    "tuning_logit = {\n",
    "    \"logit\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"preprocess\": {\n",
    "            \"standardize\":True,\n",
    "            \"pca\":False\n",
    "        },\n",
    "        \"fixed_params\" : {\n",
    "            \"logit__max_iter\":100,\n",
    "            \"logit__solver\":\"saga\", # Class weight can be an interesting parameter to check.\n",
    "            \"logit__n_jobs\":-1, # How does paralelizing the logit work??????\n",
    "            \"logit__penalty\":\"elasticnet\", \n",
    "                        }\n",
    "        ,\n",
    "        \"grid_search_params\":{\n",
    "            # Lower C is higher penalty, if optimal is not at bound the approximation is good enough\n",
    "            \"logit__C\":[0.001, 0.101, 0.201, 0.301, 0.401, 0.501, 0.601, 0.701, 0.801, 0.901, 1.001,5,10],\n",
    "            \"logit__l1_ratio\":[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], # Nests lasso and ridge both edges.\n",
    "            \"logit__class_weight\":[None,\"balanced\"] # Uses inverse probability weighting. I really don't like this\n",
    "            # Class is not that unbalanced but try it.\n",
    "            \n",
    "        }\n",
    "    }\n",
    "    \n",
    "}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tuning_qda = {\n",
    "    \"qda\": {\n",
    "        \"model\": QuadraticDiscriminantAnalysis(),\n",
    "        \"preprocess\": {\n",
    "            \"standardize\":True,\n",
    "            \"pca\":False\n",
    "        },\n",
    "        \"fixed_params\" : {\n",
    "            \"qda__priors\":None\n",
    "                        }\n",
    "        ,\n",
    "        \"grid_search_params\":{\n",
    "            # Lower C is higher penalty, if optimal is not at bound the approximation is good enough\n",
    "            \"qda__reg_param\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "            \n",
    "            \n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(1,21,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tuning_knn = {\n",
    "    \"knn\": {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"preprocess\": {\n",
    "            \"standardize\":True,\n",
    "            \"pca\":False\n",
    "        },\n",
    "        \"fixed_params\" : {\n",
    "            \"knn__metric\":'minkowski',\n",
    "            \"knn__n_jobs\":1 # Just paralelize the cv not the model itlself.\n",
    "                        }\n",
    "        ,\n",
    "        \"grid_search_params\":{\n",
    "            # This will have to be reduced it takes way too long!\n",
    "            # From 0 to 1 these are not distances, so be careful or make the test params sparser from 0 to 1\n",
    "            \"knn__p\": [0.8, 1.0, 1.2,  1.4,  1.7, 2.0, 2.2, 3.0, 10.0],#Del 0 al 2 y 3,4,10\n",
    "            \"knn__weights\":['uniform','distance'],\n",
    "            # If best near the edge we can fine tune there.\n",
    "            \"knn__n_neighbors\": [1 , 3, 5, 9, 11, 15, 20, 30]\n",
    "\n",
    "            \n",
    "            \n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tuning_svm = { \n",
    "    \"svm\": {\n",
    "        \"model\": SVC(),\n",
    "        \"preprocess\": {\n",
    "            \"standardize\":True,\n",
    "            \"pca\":False\n",
    "        },\n",
    "        \"fixed_params\" : {\n",
    "            \"svm__degree\":3                        \n",
    "        }\n",
    "        ,\n",
    "        \"grid_search_params\":{\n",
    "            # Same as in logit, if near edges we can modify it.\n",
    "            \"svm__C\": [0.001, 0.101, 0.201, 0.301, 0.401, 0.501, 0.601, 0.701, 0.801, 0.901, 1.001,5,10] ,\n",
    "            # Kernel is the most fun thing.\n",
    "            \"svm__kernel\":[\"linear\",\"poly\",\"rbf\",\"sigmoid\"],\n",
    "            # If best near the edge we can fine tune there.\n",
    "            \"svm__gamma\": [\"scale\",\"auto\"],\n",
    "            \"svm__class_weight\": [None,\"balanced\"]\n",
    "\n",
    "            #Will need to fix the random state of the model.\n",
    "            \n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tuning_rf = { \n",
    "    \"rf\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"preprocess\": {\n",
    "            \"standardize\":False,\n",
    "            \"pca\":False\n",
    "        },\n",
    "        \"fixed_params\" : {\n",
    "            # Almost in the definition of a random forest.\n",
    "            \"rf__bootstrap\":True                        \n",
    "        }\n",
    "        ,\n",
    "        \"grid_search_params\":{\n",
    "            # If expensive kick out the highest number of trees.\n",
    "            # 100 is default\n",
    "            #Not playing with tree depth or whatever, trees are going to go deep.\n",
    "            \"rf__n_estimators\": [100,200,300,400] ,\n",
    "            \"rf__criterion\":[\"gini\",\"entropy\"],\n",
    "            \"rf__max_features\":[\"sqrt\",\"log2\"] # log2 is more strict!\n",
    "            \n",
    "\n",
    "            #Will need to fix the random state of the model.\n",
    "            \n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_gbdt = { \n",
    "    # This is not working too well I feel like but whatever.\n",
    "    \"gbdt\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"preprocess\": {\n",
    "            \"standardize\":False,\n",
    "            \"pca\":False\n",
    "        },\n",
    "        \"fixed_params\" : {\n",
    "            # Almost in the definition of a random forest.\n",
    "            \"gbdt__loss\":\"log_loss\",\n",
    "            \"gbdt__n_iter_no_change\":15 # Early stopping I don't want to spend eternity.\n",
    "        }\n",
    "        ,\n",
    "        \"grid_search_params\":{\n",
    "            # If expensive kick out the highest number of trees.\n",
    "            # 100 is default\n",
    "            #Not playing with tree depth or whatever, trees are going to go deep.\n",
    "            \"gbdt__n_estimators\": [100,200,300,400,500],#[100,200], # I think in this context this is M from the book.\n",
    "            \"gbdt__max_depth\":[1,3,5,7,9],#[1,3,5], # I think this is J from the book.\n",
    "            \"gbdt__learning_rate\":[0.1,0.05,0.2,0.3],#[0.05, 0.1, 0.2], # This is v in the book I think, tradeoff between M and v\n",
    "            # I think we want small v, depends on iterations and max depth of the tree.\n",
    "            \"gbdt__max_features\":[\"sqrt\",100,130] # This param with None makes it 5 minutes per iter.\n",
    "            #[None,\"log2\"] # Default is None, but why not introduce a random forest component.\n",
    "            # subsample could also be interesting to tune.\n",
    "            \n",
    "            #This a bit useless probably, as the model with None was so powerful already. -> OH BASELINE 93.5 ok we can work with that.\n",
    "            #Will need to fix the random state of the model.\n",
    "            \n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 3 ideas:\n",
    "https://scikit-learn.org/stable/modules/ensemble.html# Ensembles are usually super powerful.\n",
    "- Adaboost in action use decision tree as base learner. We can also tune the base learner, try and iteratively fit SVC or something along those lines.\n",
    "- Random forests or gbdt can be very tuned and should yield some good estimates.\n",
    "- Use PCA somehow, test different optimal number of components.\n",
    "- t-SNE or UMAP, modern dimensionality reduction algorithms.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py\n",
    "If we can reduce dimensionality to 2 we can plot these decision boundary plots that are so cute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | PCA | Standardization | Tuning params | Explanation | Preliminary F1       |   TIME |\n",
    "|----------------|--------------|------------|---|----|-----|--------|\n",
    "| ADABOOST  |  YES     | YES  |estimator, n_estimators, learning_rate| We can try to play with the baseline estimators of the adaboost algorithm. We can sue an SVC or something like that. Try the baseline to be the trees and svc. Then we can modify the number of iterations as well as the learning rate which would be the alpha params. It may be good to keep it low as high values may lead to overfitting. SEEMS LOGISTIC AS BASELINE CAN BE GOOD| 0.93 | 10 min\n",
    "| Try ThresholdClassifierCV()  |       |   | || \n",
    "| PCA toy around justified by screeplot.  |       |   | || "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'ada_boost__estimator': LogisticRegression(), 'ada_boost__learning_rate': 0.5, 'ada_boost__n_estimators': 200, 'pca__n_components': 50}\n",
    "# 0.946\n",
    "tuning_ada_boost = { \n",
    "    # This is not working too well I feel like but whatever.\n",
    "    # The JSON will give an error which is problematic. because decision tree classifier\n",
    "    # gets returned.\n",
    "    \"ada_boost\": {\n",
    "        \"model\": AdaBoostClassifier(),\n",
    "        \"preprocess\": {\n",
    "            \"standardize\":True, # Needed for the SVC.\n",
    "            \"pca\":True\n",
    "        },\n",
    "        \"fixed_params\" : {\n",
    "            # Almost in the definition of a random forest.\n",
    "            \"ada_boost__algorithm\":\"SAMME\"\n",
    "        }\n",
    "        ,\n",
    "        \"grid_search_params\":{\n",
    "            # If expensive kick out the highest number of trees.\n",
    "            # 100 is default\n",
    "            #Not playing with tree depth or whatever, trees are going to go deep.\n",
    "            \"ada_boost__estimator\": [DecisionTreeClassifier(max_depth=2),LogisticRegression()], # Try good model as base, may overfit.\n",
    "            \"ada_boost__n_estimators\":[50,100,200,300,400], # Keep iterations low for complex model\n",
    "            \"ada_boost__learning_rate\":[0.05,0.1,0.2,0.5,0.6, 1,1.5], # This would be alpha or similar, keep low so as not to ofit.\n",
    "            \"pca__n_components\":[8,10,12,15,25]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline svm out of box. 0.95 Made so it can give back a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(random_state=42,probability=True)\n",
    "svm.fit(X_TRAIN_G,y_TRAIN_G)\n",
    "y_pred = svm.predict(X_TEST_G)\n",
    "f1_score(y_TEST_G,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9496774193548387)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_true, y_pred\n",
    "f1_score(y_TEST_G,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(solver=\"lsqr\",shrinkage=0.7)\n",
    "lda.fit(X_TRAIN_G,y_TRAIN_G)\n",
    "y_pred_lda = lda.predict(X_TEST_G)\n",
    "# F1 worsens from baseline as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9501915708812261)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_TEST_G,y_pred_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/m_learn/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.9501915708812261)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(C=0.8,class_weight=\"balanced\", penalty=\"l1\",solver=\"saga\")\n",
    "logit.fit(X_TRAIN_G,y_TRAIN_G)\n",
    "y_pred_logit = lda.predict(X_TEST_G)\n",
    "f1_score(y_pred_logit,y_TEST_G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9226666666666666)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out of box random forest Improved to 0.94\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_TRAIN_G,y_TRAIN_G)\n",
    "y_pred_rf = rf.predict(X_TEST_G)\n",
    "f1_score(y_pred_rf,y_TEST_G)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1.3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_rf_13 = { \n",
    "    # This is not working too well I feel like but whatever.\n",
    "    # The JSON will give an error which is problematic. because decision tree classifier\n",
    "    # gets returned.\n",
    "    \"rf_13\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"preprocess\": {\n",
    "            \"standardize\":True, \n",
    "            \"pca\":True\n",
    "        },\n",
    "        \"fixed_params\" : {\n",
    "            \"rf_13__criterion\":\"gini\"\n",
    "        }\n",
    "        ,\n",
    "        \"grid_search_params\":{\n",
    "            \"pca__n_components\":[5,6,7,8,9,10,11,12]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got a test of 0.956 I think that is the highest.\n",
    "tuning_ada_boost_logit = { \n",
    "    # This is not working too well I feel like but whatever.\n",
    "    # The JSON will give an error which is problematic. because decision tree classifier\n",
    "    # gets returned.\n",
    "    \"ada_boost_l\": {\n",
    "        \"model\": AdaBoostClassifier(),\n",
    "        \"preprocess\": {\n",
    "            \"standardize\":True, # Needed for the SVC.\n",
    "            \"pca\":True\n",
    "        },\n",
    "        \"fixed_params\" : {\n",
    "            # Almost in the definition of a random forest.\n",
    "            \"ada_boost_l__algorithm\":\"SAMME\"\n",
    "        }\n",
    "        ,\n",
    "        \"grid_search_params\":{\n",
    "            # If expensive kick out the highest number of trees.\n",
    "            # 100 is default\n",
    "            #Not playing with tree depth or whatever, trees are going to go deep.\n",
    "            \"ada_boost_l__estimator\": [LogisticRegression()], # Try good model as base, may overfit.\n",
    "            \"ada_boost_l__n_estimators\":[100,200,300,400], # Keep iterations low for complex model\n",
    "            \"ada_boost_l__learning_rate\":[0.005,0.01,0.02,0.05,0.1,0.2], # This would be alpha or similar, keep low so as not to ofit.\n",
    "            \"pca__n_components\":[8,10,12,15,25]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got a test of 0.958 I think that is the highest.\n",
    "# \tparams {'logit_13__C': 0.2, 'logit_13__l1_ratio': 0.6, 'pca__n_components': 25} 2\n",
    "# \n",
    "tuning_logit13 = { \n",
    "    # This is not working too well I feel like but whatever.\n",
    "    # The JSON will give an error which is problematic. because decision tree classifier\n",
    "    # gets returned.\n",
    "    \"logit_13\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"preprocess\": {\n",
    "            \"standardize\":True, # Needed for the SVC.\n",
    "            \"pca\":True\n",
    "        },\n",
    "        \"fixed_params\" : {\n",
    "            # Almost in the definition of a random forest.\n",
    "            \"logit_13__solver\":\"saga\",\n",
    "            \"logit_13__penalty\": \"elasticnet\",\n",
    "            \"logit_13__max_iter\": 200\n",
    "        }\n",
    "        ,\n",
    "        \"grid_search_params\":{\n",
    "            # If expensive kick out the highest number of trees.\n",
    "            # 100 is default\n",
    "            #Not playing with tree depth or whatever, trees are going to go deep.\n",
    "            \"logit_13__C\": [0,0.1,0.2,0.3], # Try good model as base, may overfit.\n",
    "            \"logit_13__l1_ratio\":[0,0.2,0.4,0.6,0.8,1],\n",
    "            \"pca__n_components\":[8,10,12,15,25,30,35,40,45,60,70,80,90,100],\n",
    "            \"logit_13__class_weight\":[\"balanced\",None]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model: logit_13\n",
      "Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=10)),\n",
      "                ('logit_13',\n",
      "                 LogisticRegression(max_iter=200, penalty='elasticnet',\n",
      "                                    solver='saga'))])\n",
      "Searching over the following parameters:{'logit_13__C': [0, 0.1, 0.2, 0.3], 'logit_13__l1_ratio': [0, 0.2, 0.4, 0.6, 0.8, 1], 'pca__n_components': [8, 10, 12, 15, 25, 30, 35, 40, 45, 60, 70, 80, 90, 100], 'logit_13__class_weight': ['balanced', None]}\n",
      "Fitting 5 folds for each of 672 candidates, totalling 3360 fits\n"
     ]
    }
   ],
   "source": [
    "CV_FOLD = 5\n",
    "SCORING = \"f1\"\n",
    "N_JOBS = -1 # Means we use paralel computing across folds to accelerate muajajaja.\n",
    "# For test it reduced 4 minutes, out of 17 so not too bad.\n",
    "# For verbosity we would ideally want a global indicator like percentage of fits done and a time\n",
    "\n",
    "#I will need to fix the random state of the cross validation inside the GridSearch\n",
    "\n",
    "\n",
    "for model, params in tuning_logit13.items():\n",
    "    print(\"Tuning model: \" +model)\n",
    "    \n",
    "    pipeline = pipe_maker(model_name=model,\n",
    "                       model = params[\"model\"],\n",
    "                       standardize=params[\"preprocess\"][\"standardize\"],\n",
    "                       with_pca=params[\"preprocess\"][\"pca\"] \n",
    "                      )\n",
    "    \n",
    "    # Setting parameters that will not be grid searched.\n",
    "    pipeline.set_params(**params[\"fixed_params\"])\n",
    "    print(pipeline)\n",
    "\n",
    "    # Scoring to maximize the F1 score, cv 5 fold. \n",
    "    print(f\"Searching over the following parameters:{params[\"grid_search_params\"]}\")\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid=params[\"grid_search_params\"],scoring=SCORING,cv=CV_FOLD,n_jobs=N_JOBS,\n",
    "                               verbose = 1 #More information.\n",
    "                               # Try one I want 300/1500 fits completed\n",
    "                              \n",
    "                              )\n",
    "\n",
    "    #  REMEMBER: This comes from a fixed random state =42, we can't hyper parameter tune on the test set! \n",
    "    # This training set is then fixed, all final check should be conducted on test based on random split\n",
    "    tic =time.time()\n",
    "\n",
    "    grid_search.fit(X_TRAIN_G,y_TRAIN_G)\n",
    "\n",
    "    toc = time.time()\n",
    "    \n",
    "    exec_time = (toc-tic)/60\n",
    "    \n",
    "    # Export results. Full thing to csv. Dictionary of optimal params to csv\n",
    "    full_results = pd.DataFrame(grid_search.cv_results_)\n",
    "    full_results.to_csv(\"Tuning_params/\"+model+\".csv\")\n",
    "\n",
    "    # Params to json\n",
    "    optimal_params_to_json = grid_search.best_params_.copy()\n",
    "    optimal_params_to_json[\"MODEL_NAME\"] = model\n",
    "    optimal_params_to_json[\"DATE\"] = datetime.datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    optimal_params_to_json[\"TIME_ELAPSED\"] = exec_time\n",
    "    #FOR ADABOOST NEED TO COMMENT THIS OR GIVES ERROR.\n",
    "    #hyper_param(path=\"Tuning_params/\"+model+\".json\",params=optimal_params_to_json)\n",
    "    \n",
    "    # Succesful\n",
    "    print(\"Hypertuning of: \" +model+ f\". Time elapsed: {exec_time} minutes\")\n",
    "    y_test_pred = grid_search.predict(X_TEST_G)\n",
    "    print(f\"F1 on test set: {f1_score(y_TEST_G,y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can't go to a 0.96, is the grid search being f1 making things worse?\n",
    "\n",
    "It could be that this grid search param makes the model good f1 for the optimal threshold but then not for the different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9565707008043924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.9278350515463918)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.time()\n",
    "gbdt = GradientBoostingClassifier(random_state=42)\n",
    "gbdt.fit(X_TRAIN_G,y_TRAIN_G)\n",
    "y_pred = gbdt.predict(X_TEST_G)\n",
    "toc = time.time()\n",
    "print((toc-tic)/60)\n",
    "f1_score(y_TEST_G,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.756601746877035 minutes to run.\n",
      "0.4459125820454244 0.9323714883994777\n",
      "F1 on test set after thresholding: 0.9326556543837357\n"
     ]
    }
   ],
   "source": [
    "#grid_search.best_estimator_\n",
    "\n",
    "threshold_modifier = TunedThresholdClassifierCV(gbdt, scoring=\"f1\",random_state=42,store_cv_results=True)\n",
    "\n",
    "tic = time.time()\n",
    "threshold_modifier.fit(X_TRAIN_G,y_TRAIN_G)\n",
    "\n",
    "toc = time.time()\n",
    "print(f\"{(toc-tic)/60} minutes to run.\")\n",
    "\n",
    "print(threshold_modifier.best_threshold_, threshold_modifier.best_score_)\n",
    "\n",
    "y_pred_t = threshold_modifier.predict(X_TEST_G)\n",
    "\n",
    "print(f\"F1 on test set after thresholding: {f1_score(y_TEST_G, y_pred_t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'F1 score vs. Decision threshold -- Cross-validation')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://towardsdatascience.com/tune-in-decision-threshold-optimization-with-scikit-learns-tunedthresholdclassifiercv-7de558a2cf58\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(\n",
    "    threshold_modifier.cv_results_[\"thresholds\"],\n",
    "    threshold_modifier.cv_results_[\"scores\"],\n",
    "    marker=\"o\",\n",
    "    linewidth=1e-3,\n",
    "    markersize=1,\n",
    "    color=\"#c0c0c0\",\n",
    ")\n",
    "ax.plot(\n",
    "    threshold_modifier.best_threshold_,\n",
    "    threshold_modifier.best_score_,\n",
    "    \"^\",\n",
    "    markersize=10,\n",
    "    color=\"#ff6700\",\n",
    "    label=f\"Optimal cut-off point = {threshold_modifier.best_threshold_:.2f}\",\n",
    ")\n",
    "ax.axhline(f1_score(y_TEST_G, y_pred_t), label =\"F1 on test after thresholding\")\n",
    "\n",
    "ax.axhline(0.96, label =\"0.96 where we want to go\")\n",
    "\n",
    "\n",
    "ax.plot(\n",
    "    0.5,\n",
    "    0.90,\n",
    "    label=\"Default threshold: 0.5, not true for all models eh\",\n",
    "    color=\"#004e98\",\n",
    "    linestyle=\"--\",\n",
    "    marker=\"X\",\n",
    "    markersize=10,\n",
    ")\n",
    "ax.legend(fontsize=8, loc=\"lower center\")\n",
    "ax.set_xlabel(\"Decision threshold\", fontsize=10)\n",
    "ax.set_ylabel(\"F1 score\", fontsize=10)\n",
    "ax.set_title(\"F1 score vs. Decision threshold -- Cross-validation\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning_voting = { \n",
    "#     # This is not working too well I feel like but whatever.\n",
    "#     # The JSON will give an error which is problematic. because decision tree classifier\n",
    "#     # gets returned.\n",
    "#     \"voting\": {\n",
    "#         \"model\": VotingClassifier([\n",
    "#         ('lr', LogisticRegression()), ('svm', SVC(probability=True)), ('rf', RandomForestClassifier())\n",
    "#             ]),\n",
    "#         \"preprocess\": {\n",
    "#             \"standardize\":True, # Needed for the SVC.\n",
    "#             \"pca\":True\n",
    "#         },\n",
    "#         \"fixed_params\" : {\n",
    "#             # Almost in the definition of a random forest.\n",
    "#             \"voting__weights\":None\n",
    "#         }\n",
    "#         ,\n",
    "#         \"grid_search_params\":{\n",
    "#             # If expensive kick out the highest number of trees.\n",
    "#             # 100 is default\n",
    "#             #Not playing with tree depth or whatever, trees are going to go deep.\n",
    "#          # Models should be calibrated already.\n",
    "#             \"voting__voting\":[\"hard\",\"soft\"],\n",
    "#             \"pca__n_components\":[10,100]\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
