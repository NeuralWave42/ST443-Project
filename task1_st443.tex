\documentclass{article}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}


\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{siunitx}

\usepackage{graphicx} % Required for inserting images
\usepackage{algpseudocodex}
% \usepackage[preprint]{neurips_2024}
\usepackage[preprint]{neurips_2024}
\begin{document}

\section{1.1 Exploratory data analysis}

\textbf{REMOVE THESE AUTOMATIC INDICES FROM OVERLEAF CAN BE CONFUSING}
\textbf{WE SHOULD HAVE AN ABSTRACT, add citations as well, website and book from my part.}


\subsection{Basic facts}
We have a dataset that consists of 5471 (n) samples with 4124 (p) columns. The features of our data are all continuous, in a logarithmic scale and they expression levels 
for genes. It is worth mentioning that the dataset is sparse, that is, a lot of cells have many gene expressions that are 0. While it is hard to have an objective measure
of sparsity we can plot average gene expression levels across all our features, see Figure \ref{fig:hist_sparse} below, most genes are non zero for a limited number of cells.

\begin{figure}[h]

    
    \caption{Possible figure of average gene expression level across all genes. We can do either histogram or histogram by label.}\label{fig:hist_sparse}
\end{figure}


We have 2 distinct classes of cells, the TREG cells and the CD4+T cells. 
These are going to be our labels that we want to classify. We have some class imbalance, the ratio is 6/10 in favour of the CD4+T, 
which is the dominant class. This is going to be important for the models we try to tune as some models have options to adjust for class imbalance.
We will also try the option of tuning the threshold decision for classifying to one cell or another. By default the probabilistic models in Scikit-learn
classify to the positive class if the conditional probability for the given model $\mathbb{P}(\textit{y}|X) > 0.5$\footnote{\href{https://scikit-learn.org/1.5/modules/classification_threshold.html}{see scikit references}}. 
We will tinker with this threshold to try to optimize the F1-Score given our class imbalance.  

\subsection{Visualization and dimensionality redcution}

Both the fact that $n \approx p$ and the fact that the data is sparse point us towards using towards using regularization and feature selection.
The instructions for this problem also make us use PCA with 10 components. To confirm whether the number of components is optimal, we can plot the cumulative sum of the explained variance by each of the components.
As you can see below in Figure \ref{fig:scree}, choosing just the first ten components makes us use a very amount of the total variance.
In Section \ref{sec:1.3} we will try to tune the number of components to get an improved F1-Score.

\begin{figure}[h]
    
    
    \caption{Scree plot: cumulative sum of PCA components.}\label{fig:scree}
\end{figure}

For completeness, we add Figure \ref{fig:tsne} where we use t-SNE to reduce the dimensionality of the data from 4124 to 2\footnote{ The underlying algorithm is stochastic 
and quite sensitive to how we tune the hyperparameter of perplexity} with the purpose of visualizing the joint distribution (after the t-SNE transformation) of both cells. 
Figure \ref{fig:tsne} shows some separability of the two classes. It also shows some cells of a given clas (TREG) in regions where the density is much higher for the other type of cell (CD4T), maybe we can use this as intuition as to why
in later sections we find it difficult to improve the F1-Score beyond 0.95.


\begin{figure}[h]
    
    
    \caption{Joint distribution of transformed data by t-SNE}\label{fig:tsne}
\end{figure}
% Make sure we explain why it doesn't work. -> T-sne or something like that.
% Correct way to use t-sne if we have space is to plot different perplexities. TRY DO IT AND JUST MENTION IT IN A NOTE.
% \section{Training baseline models and hypertuning}

\section{1.2 Training, tuning and evaluating baseline models.}
This section is split into two parts, the first part describes our code from Python at a high-level. It focuses on some of scikit-learn procedures to streamline the analysis 
and guarantee good test performance. Then we move to explain which parameters we tune to improve the performance of the baseline models.

\subsection{Using the scikit-learn toolkit.}
Because we are going do to some hyperparameter tuning we first need to split the data into a test and train subset -this is needed because we tuned some hyperparameters, if not an estimate of test error obtained through cross validation should be valid -\footnote{The \href{https://scikit-learn.org/1.5/modules/cross_validation.html}{scikit documentation} contain a very understandable graph of the workflow.}

To do the hyperparameter tuning we implemented grid search\footnote{The scikit documentation on \href{https://scikit-learn.org/1.5/modules/grid_search.html}{grid search.}
} which is a brute force approach that consists of trying all hyperparameter combinations specified by the researcher. 
We use the training set for a given set of parameters, we use cross validation -a part of our training data is left out as a validations set, the model is trained on the rest of the training data and we get a validation error estimate, this process is repeated 5 times - and obtain an estimate of the metric we are trying to optimize over.
We iterate over all parameter combinations and pick the one that maximizes the metric we choose (in part 1 it was F1 score).

Some models need the data to be standarized, and we are also asked to use PCA, we therefore make use of pipelines \footnote{The scikit library has great documentation:
\href{https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html}{Pipeline} includes some case uses.}, which are a sequence of data preprocessing steps with a final estimator. Using this 
streamlines our analysis and avoids some \href{https://scikit-learn.org/stable/common_pitfalls.html}{common pitfalls.}
Example: Our cross validation estimator has selected a random subset of the training data, then the pipeline will: Standarize the data $\rightarrow$ Apply PCA $\rightarrow$ Fit the classifier on the preprocessed data.

Finally, throughout our analysis we set a random seed so that our results are reproducible.

\subsection{Hyperparameter tuning the baseline models.}
We focus on tuning the models without PCA, as that is where most of the gains can be made. Due to the relatively high dimensionality of the
data some models are underperfoming if we use all the features avalable. In contrast even the out-of-the-box models without hyperparameter tuning 
perform really well when training on the first 10 components of the PCA. Therefore we compare the tuned models without PCA to the untuned models with PCA. 
Next we show what we tuned and the results we obtained.

\subsubsection{What did we tune?}

For the first 3 models -\textit{LDA, Logistic regression and QDA}- the tuning is mostly focused on imposing strong penalization mechanisms, such as lasso, ridge or elastic net.

For \textit{k-NN}, there are a lot of interesting parameters. We search across different numbers of neighbors. We also tune across rules to determine who the nearest
neighbors are. To achieve this we use the Minkowski(p) distance, this is a distance that nests the classical euclidean (Minkowski($p_1$)) and manhattan distances (Minkowski($p_2$)), so we search across p to try different distances.
Finally we also try to compute the decision rule in different weights, the default where all neighbors have the same weight in the vote and one where the votes from the closest neighbors within the neighborhood carry more weight.

For \textit{Support Vector Machine}, we tune across different kernels and regularization strenghts. The default uses the radial/gaussin kernel, we also try to use linear kernels as well as some others we haven't seen in class.

For \textit{Random Forests} our grid search is not extensive, we tune the criterion made to determine the quality of a split (gini and entropy), the number of trees in the forest and the number of features a tree in the forest will choose from.
It is worth mentioning that we considered pruning, as we saw the method in class, however we decide against it as it would be computationally expensive.

For \textit{Gradient Boosted Decision Trees (GBDT)} we consider different loss functions, number of iterations, learning rates, as well as the depth and features used by the trees. 
We also set an early stopping rule as this grid search is computationally expensive.
For other models the tuning parameters are mostly self explanatory, in this case it is more complex.
For intuition let us remember that this is similar to AdaBoost, learning determines how new iterations influence the final vote and the tree complexity should be tuned to avoid overfitting.
If we run too many iterations with a deep tree we might overfit, while if we run too few iterations on shallow trees, our performance may be suboptimal.
Finding the optimal between iterations, learning and tree complexity is complex.

Finally, across our tuning, we  use tuning paramters related to class weights were available. As while the classes are not as imbalanced as in Task 2, there is some imbalance in favour of the CD4+T.
    


\subsubsection{Analysis of results}

We compare our fully tuned models without PCA to out of the box models with PCA. As you can see from Table 1 below
out-of-box with PCA have similar or better performance to the ones we fully tune without PCA.
The models most affected by PCA are LDA, QDA and \textit{k-NN}. Using PCA improves the perfomance of QDA and K-NN dramatically, 
for Logisitc regression and LDA the performance is not improved too much as the optimal models from the tuning already contain severe forms of regularization,
which is very similar in spirit to what PCA is doing.



\begin{table}[h]
    \begin{tabular}{SSSSSSSS} \toprule
        {$m$} & {$\Re\{\underline{\mathfrak{X}}(m)\}$} & {$-\Im\{\underline{\mathfrak{X}}(m)\}$} & {$\mathfrak{X}(m)$} & {$\frac{\mathfrak{X}(m)}{23}$} & {$A_m$} & {$\varphi(m)\ /\ ^{\circ}$} & {$\varphi_m\ /\ ^{\circ}$} \\ \midrule
        1  & 16.128 & +8.872 & 16.128 & 1.402 & 1.373 & -146.6 & -137.6 \\
        2  & 3.442  & -2.509 & 3.442  & 0.299 & 0.343 & 133.2  & 152.4  \\
        3  & 1.826  & -0.363 & 1.826  & 0.159 & 0.119 & 168.5  & -161.1 \\
        4  & 0.993  & -0.429 & 0.993  & 0.086 & 0.08  & 25.6   & 90     \\ \midrule
        5  & 1.29   & +0.099 & 1.29   & 0.112 & 0.097 & -175.6 & -114.7 \\
        6  & 0.483  & -0.183 & 0.483  & 0.042 & 0.063 & 22.3   & 122.5  \\
        7  & 0.766  & -0.475 & 0.766  & 0.067 & 0.039 & 141.6  & -122   \\
        8  & 0.624  & +0.365 & 0.624  & 0.054 & 0.04  & -35.7  & 90     \\ \midrule
        9  & 0.641  & -0.466 & 0.641  & 0.056 & 0.045 & 133.3  & -106.3 \\
        10 & 0.45   & +0.421 & 0.45   & 0.039 & 0.034 & -69.4  & 110.9  \\
        11 & 0.598  & -0.597 & 0.598  & 0.052 & 0.025 & 92.3   & -109.3 \\ \bottomrule
    \end{tabular}
\caption{Fully tuned models with PCA vs out-of-box with PCA.}
\label{tab:baseline}        
\end{table} 




\section{Our 3 models} \label{sec:1.3}

Explanation of your approaches 15 points!
\begin{itemize}
    \item AdaBoost, just because we have seen it in class and it could be interesting.
    \item Play around with the PCA optimality. Logit with tuned PCA and everything else.
    \item Threshold optimization. 
\end{itemize}

\end{document}